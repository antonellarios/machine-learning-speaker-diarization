# ğŸ”Š Speaker Diarization â€“ Final Machine Learning Project  
### Custom ResNet50 Embedding Model + Clustering  
### Author: Antonella RÃ­os  

---

## ğŸ§  Overview

This project implements **Speaker Diarization**, answering â€œwho spoke when?â€ using a **fully custom machine learning pipeline**, without PyAnnote end-to-end models.

The system includes:

- Custom **ResNet50** audio embedding network  
- Mel-spectrogram generation  
- Sliding-window segmentation  
- Embedding extraction  
- Agglomerative clustering  
- DER/JER evaluation  
- External audio inference  

Everything is coded manually using **PyTorch, Librosa, NumPy, Scikit-Learn**.

---

## ğŸ¯ Project Goals

- Train a custom ResNet50 to generate speaker embeddings  
- Build diarization from scratch with clustering  
- Compute DER & JER on VoxConverse  
- Support inference on external MP3/WAV  
- Ensure reproducibility with Drive + checkpoints  

---

## ğŸ“ Project Structure

~~~plaintext
TP_FINAL_DIARIZATION/
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ voxconverse/
â”‚       â”œâ”€â”€ audio/
â”‚       â”œâ”€â”€ rttm/
â”‚       â””â”€â”€ metadata/
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ training_logs/
â”‚   â””â”€â”€ clustering/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ diarization_pipeline.ipynb
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ diarization_outputs/
    â”œâ”€â”€ rttm_predictions/
    â””â”€â”€ metrics/
~~~

---

## ğŸ”§ Technical Pipeline (exact implementation)

### **1. Audio Preprocessing**
- Loaded with librosa  
- Resampled to 16 kHz  
- Converted to mono  
- Sliding windows:
  - Window: **1.5 s**
  - Hop: **0.75 s**

---

## **2. Mel-Spectrogram Generation**
Consistent preprocessing:

- Mel filterbank  
- Log-scaled  
- Normalized  
- Ready for CNN input  

---

## **3. Custom ResNet50 Embedding Model**

The ResNet50 has been **modified**:

- First Conv layer changed to accept **1-channel** input  
- Global feature map â†’ Flatten  
- Fully Connected layer **2048 â†’ 512**  
- BatchNorm(512)  
- Dropout(0.3)  
- L2-normalized embeddings  

Optimized with:

- Adam optimizer  
- CrossEntropy loss  
- Optional AMP (mixed precision)  
- Epoch logging and validation  

Checkpoint saved in:

~~~plaintext
/checkpoints/model/resnet50_speaker_embeddings.pth
~~~

---

## **4. Training Process**
Using VoxConverse speaker segments:

- Balanced batches  
- Spectrogram augmentation  
- Validation on each epoch  
- GPU training (Colab)  

---

## **5. Embedding Extraction**
For each audio file:

- Apply sliding windows  
- Convert window â†’ mel-spec  
- Feed to ResNet50  
- Store embeddings:

~~~plaintext
/checkpoints/embeddings/{audio_id}.npy
~~~

---

## **6. Clustering**
Agglomerative clustering (cosine affinity):

~~~plaintext
/checkpoints/clustering/{audio_id}.npy
~~~

Creates the speaker groups that form diarization.

---

## **7. Diarization Assembly**
- Map windowâ†’speaker label  
- Merge continuous segments  
- Export RTTM-style prediction:

~~~plaintext
/results/rttm_predictions/{audio_id}.rttm
~~~

---

## ğŸ§ª Evaluation (DER & JER)

Two metrics implemented:

- **DER (Diarization Error Rate)**
- **JER (Jaccard Error Rate)**

Formula:

~~~plaintext
DER = (False Alarm + Missed Speech + Confusion) / Total Speech
~~~

Results saved in:

~~~plaintext
/results/metrics/{audio_id}_metrics.json
~~~

---

## ğŸ Challenges & Solutions

### âœ” Colab resets  
â†’ Checkpoints in Drive + `os.path.exists()` guards

### âœ” Long training/inference  
â†’ Windowed processing + smaller test subsets

### âœ” RTTM alignment issues  
â†’ Manual inspection + consistent loader logic

### âœ” Model compatibility  
â†’ Forced `numpy==1.26.4` and correct Torch+Librosa versions  

---

## ğŸ§ External Audio Inference

Your model supports **any MP3/WAV**:

- Load file  
- Window segmentation  
- Spectrogram  
- Embedding extraction  
- Clustering  
- Final diarization output  

Example console output:

~~~plaintext
Speaker 0 â†’ 00:00:00 - 00:00:12
Speaker 1 â†’ 00:00:12 - 00:00:28
Speaker 0 â†’ 00:00:28 - 00:00:41
~~~

Example RTTM:

~~~plaintext
file1 1 SPEAKER 0.00 12.00 <NA> <NA> 0 <NA> <NA>
file1 1 SPEAKER 12.00 16.00 <NA> <NA> 1 <NA> <NA>
~~~

---

## ğŸ§© How to Run This Project

### **1. Mount Drive**
~~~python
from google.colab import drive
drive.mount('/content/drive')
~~~

### **2. Install Dependencies**
~~~bash
pip install torch torchvision torchaudio
pip install librosa
pip install scikit-learn
pip install numpy==1.26.4
~~~

### **3. Run the Notebook Sections**
1. Imports & utils  
2. Preprocessing  
3. Spectrograms  
4. ResNet50 model  
5. Training  
6. Embeddings  
7. Clustering  
8. Diarization  
9. Evaluation  
10. External audio test  

---

## ğŸ‘©â€ğŸ’» Author

**Antonella RÃ­os**  
Junior Data Analyst & Data Scientist Trainee  
ğŸ“ Salta, Argentina  
ğŸ“§ antonella.datasolutions@gmail.com  
ğŸ”— linkedin.com/in/antonellarios  

---


---


Antonella RÃ­os
Junior Data Analyst & Data Scientist (Trainee)
ğŸ“ Salta, Argentina
ğŸ“§ antonella.datasolutions@gmail.com

ğŸ”— linkedin.com/in/antonellarios
