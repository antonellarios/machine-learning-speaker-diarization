# -*- coding: utf-8 -*-
"""TP FINAL MACHINE LEARNING SPEAKER DIARIZATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NDeNxsNWqRLBw4EEYvTHleWItRv2Iq5k

# **Trabajo Práctico Final – Speaker Diarization con VoxConverse + ResNet50**

## **Tecnicatura Universitaria en Ciencia de Datos e Inteligencia Artificial Aplicada**  
### **Materia: Machine Learning**  
### **Docente: León Juárez **  
### **Alumna:**  
- Ríos Tejerina Melisa Antonella

---

## **Descripción General**
Este trabajo implementa un sistema completo de **diarización de hablantes** utilizando:

- Dataset **VoxConverse**  
- Segmentación basada en **archivos RTTM**  
- Generación de **embeddings** mediante una **ResNet50 adaptada**  
- **Clustering jerárquico** para asignación de hablantes  
- Evaluación mediante métricas estándares (**DER / JER**)  
- Testing final sobre audios reales (YouTube)


---

# **BLOQUE 1 — Configuración del Entorno e Instalación de Librerías**

En este bloque se configurará el entorno de ejecución de la sieguiente manera:

- Verificación de la versión de Python.
- Instalación de `pyannote.audio==3.1.1`.
- Instalación de `ipython==7.34.0`.
- Instalación de `numpy<2` para evitar incompatibilidades.
- Reinicio del entorno.
- Prueba técnica utilizando el modelo preentrenado de PyAnnote.
- Autenticación con HuggingFace para habilitar la descarga de pipelines.

### **Importación de Módulos del Sistema**

Para garantizar la correcta configuración del entorno de ejecución, importamos las librerías estándar `sys` y `subprocess`.

* **`sys`**: Nos permitirá verificar la versión del intérprete de Python y gestionar variables del entorno, asegurando la compatibilidad con las librerías de *PyAnnote*.
* **`subprocess`**: Habilita la ejecución de comandos de terminal directamente desde el script, lo cual es útil para gestionar instalaciones de paquetes (`pip`) o ejecutar procesos del sistema operativo de manera controlada.
"""

import sys
import subprocess

"""## **Verificación de la Versión de Python**

Antes de proceder con la instalación de las librerías, es fundamental verificar la versión del intérprete de Python activo en el entorno de Google Colab.

Esto es crítico porque la librería `pyannote.audio` (versión 3.1.1) tiene requisitos específicos de compatibilidad. Confirmar que estamos operando sobre una versión adecuada (generalmente Python 3.10 o superior) previene conflictos de dependencias más adelante.
"""

!python -V

"""### **Instalación de Librerías Principales**

Siguiendo los requisitos estrictos para el correcto funcionamiento en Google Colab, procedemos a instalar las librerías núcleo con sus versiones específicas.

* **`pyannote.audio==3.1.1`**: Es la librería fundamental para este trabajo práctico. Proporciona las herramientas necesarias para la diarización de hablantes, el acceso a modelos preentrenados y el procesamiento de audio.
* **`ipython==7.34.0`**: Se fija esta versión específica de IPython para garantizar la compatibilidad con el entorno de visualización y reproducción de audio en el notebook, evitando conflictos con las actualizaciones automáticas de Colab.

> **Importante:** La instalación debe realizarse en este orden estricto para evitar conflictos de dependencias que impedirían completar el trabajo.
"""

!pip install pyannote.audio==3.1.1
!pip install -qq ipython==7.34.0

"""## **Ajuste de Versión de NumPy**

Para evitar incompatibilidades críticas conocidas entre las versiones más recientes de *NumPy* (2.x) y las librerías de audio utilizadas en este proyecto, es necesario forzar el uso de una versión anterior.

* **`numpy<2`**: Esta instrucción asegura que se instale la última versión disponible de la serie 1.x (generalmente 1.26.x). Esto es vital porque `pyannote.audio` y otras dependencias aún no han migrado completamente a la sintaxis de NumPy 2.0, y el uso de la versión nueva provocaría errores de ejecución inmediatos.
* **`--upgrade`**: Garantiza que, si Colab tiene preinstalada una versión diferente que cause conflicto, esta sea reemplazada por la versión compatible que estamos solicitando.
"""

!pip install "numpy<2" --upgrade

"""## **Importación de Librerías y Verificación del Entorno**

Una vez reiniciado el entorno de ejecución, procedemos a importar las librerías fundamentales para el desarrollo del trabajo práctico. Este paso cumple una doble función: cargar las herramientas necesarias y **verificar que no existan conflictos** de instalación.

* **`torch`**: Importa PyTorch, el framework de Deep Learning sobre el que se construyen los modelos de embeddings y la arquitectura de *pyannote*.
* **`numpy`**: Librería esencial para el manejo de matrices y procesamiento numérico. Su correcta importación confirma que la versión instalada es compatible (v1.x).
* **`pyannote.audio.Pipeline`**: Clase principal que nos permitirá acceder a los pipelines de diarización pre-entrenados.
* **`ProgressHook`**: Utilidad para visualizar barras de progreso durante el procesamiento de audio.

> **Verificación:** Si este bloque se ejecuta sin errores, confirma que la instalación de `pyannote.audio` y el ajuste de versión de `numpy` fueron exitosos.
"""

import torch
import numpy as np
from pyannote.audio import Pipeline
from pyannote.audio.pipelines.utils.hook import ProgressHook

"""#  **Verificación después del Reinicio**

"""

# ===== BLOQUE 1 — Verificación =====

import torch
import numpy as np
from pyannote.audio import Pipeline
from pyannote.audio.pipelines.utils.hook import ProgressHook

print(" PyTorch cargado correctamente.")
print(" NumPy versión:", np.__version__)
print(" PyAnnote importado correctamente.")

print("\n Entorno configurado con éxito.")

"""#  **Autenticación con HuggingFace**

Para utilizar los modelos preentrenados de PyAnnote es necesario autenticarse con un token personal de HuggingFace.

1. Crear una cuenta en https://huggingface.co  
2. Confirmar el email  
3. Ir a *Settings → Access Tokens → New Token*  
4. Crear un token con permisos **Read**  
5. A continuación pegarlo cuando Colab lo pida

"""

from huggingface_hub import notebook_login

print(" Ejecutando autenticación...")
notebook_login()

"""# **Prueba de funcionamiento con el audio DEMO del dataset**

Aquí verificamos PyAnnote ejecutando un caso de diarización con un audio de ejemplo.
A continuación se descargarán:

- un audio WAV de ejemplo  
- su archivo RTTM (ground truth)

Luego se visualizará un segmento del audio y se aplicará diarización.

"""

# ===== BLOQUE 1 — DEMO PyAnnote =====

print(" Descargando audio DEMO...")
!wget -q http://groups.inf.ed.ac.uk/ami/AMICorpusMirror/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav

print(" Descargando groundtruth RTTM...")
!wget -q https://raw.githubusercontent.com/pyannote/AMI-diarization-setup/main/only_words/rttms/test/ES2004a.rttm

# Cargar groundtruth
from pyannote.database.util import load_rttm
_, groundtruth = load_rttm('ES2004a.rttm').popitem()

print(" Groundtruth cargado correctamente.")


# Visualización de un segmento
from pyannote.core import Segment, notebook
EXCERPT = Segment(600, 660)
notebook.crop = EXCERPT

print(" Visualizando anotaciones del segmento...")
groundtruth

from pyannote.audio import Audio
from IPython.display import Audio as IPythonAudio
import torchaudio

# Leer audio demo
waveform, sr = torchaudio.load("ES2004a.Mix-Headset.wav")
IPythonAudio(waveform.flatten(), rate=sr)

# Ejecutar el pipeline preentrenado (DEMO)

pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=True
)

print(" Procesando diarización DEMO...")

from pyannote.audio.pipelines.utils.hook import ProgressHook
with ProgressHook() as hook:
    diarization = pipeline(
        {"uri": "ES2004a", "audio": "ES2004a.Mix-Headset.wav"},
        hook=hook
    )

diarization

"""# **BLOQUE 2 — Montaje de Drive y Configuración de Rutas**

En este bloque se establece la conexión con Google Drive y se define la **estructura de directorios definitiva** para el proyecto.

A diferencia de configuraciones temporales, aquí definiremos rutas absolutas que apuntan directamente a Google Drive. Esto garantiza que:
1.  El dataset **VoxConverse** se descargue y descomprima en una ubicación persistente.
2.  Los **checkpoints** del modelo se guarden automáticamente para evitar perder horas de entrenamiento.
3.  Los resultados (**predicciones RTTM**) queden accesibles para su posterior evaluación.

### **Estructura del Proyecto**
Se crearán las siguientes carpetas:

- `ROOT_DIR`: Carpeta raíz del TP Final.
- `DATA_DIR`: Almacenamiento de datasets (VoxConverse).
- `CKPT_DIR`: Guardado de modelos (`.pt`), estados de entrenamiento y pickles.
- `RTTM_PRED_DIR`: Salida de las predicciones de diarización.
- `YOUTUBE_DIR`: Carpeta para pruebas con audios externos.
"""

# ===== BLOQUE 2 — Montaje de Drive y Rutas DEFINITIVAS =====

import os
from google.colab import drive

# 1. Montaje de Google Drive
print("Montando Google Drive...")
drive.mount('/content/drive', force_remount=True)
print("Drive montado correctamente.\n")

# 2. Definición de Rutas ABSOLUTAS (Configuración Maestra)
# Estas variables se usarán en todos los bloques siguientes.

ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
DATA_DIR = f"{ROOT_DIR}/datasets"
CKPT_DIR = f"{ROOT_DIR}/checkpoints"
RTTM_PRED_DIR = f"{ROOT_DIR}/rttm_pred"
YOUTUBE_DIR = f"{ROOT_DIR}/youtube_tests"

# Definición específica para VoxConverse
# NOTA: Los audios vivirán permanentemente en Drive, no en /content/

VOX_EXTRACTED_DIR = f"{DATA_DIR}/voxconverse"
AUDIO_DEV_DIR = f"{VOX_EXTRACTED_DIR}/audio"  # <-- (Antes decía /audio/dev)
RTTM_DEV_DIR  = f"{VOX_EXTRACTED_DIR}/rttm/dev"

# 3. Creación de estructura
dirs = [ROOT_DIR, DATA_DIR, CKPT_DIR, RTTM_PRED_DIR, YOUTUBE_DIR, VOX_EXTRACTED_DIR, RTTM_DEV_DIR]

print("Verificando estructura de directorios...")
for d in dirs:
    os.makedirs(d, exist_ok=True)

print(f"\nRutas configuradas.")
print(f"   Audios apuntan a: {AUDIO_DEV_DIR}")

"""### **Verificación de la Configuración**

La siguiente celda imprime un resumen de las rutas configuradas.
> **Importante:** Verifico que `AUDIO_DEV_DIR` apunte a una ruta dentro de `/content/drive/...` y **no** a una ruta temporal.
"""

# ===== BLOQUE 2 — Verificación Visual =====

print("="*40)
print(" RESUMEN DE CONFIGURACIÓN DE RUTAS")
print("="*40)

print(f"RAIZ DEL PROYECTO:   {ROOT_DIR}")
print(f"CHECKPOINTS:        {CKPT_DIR}")
print(f"AUDIOS (Target):    {AUDIO_DEV_DIR}")
print(f"RTTM (GroundTruth): {RTTM_DEV_DIR}")
print("="*40)

print("\nBLOQUE 2 COMPLETADO EXITOSAMENTE.")

"""# **BLOQUE 3 — Descarga y Preparación del Dataset VoxConverse**

En este bloque gestiono la obtención del dataset. El objetivo es alojar los archivos de audio y las anotaciones (`.rttm`) en las carpetas permanentes de Google Drive definidas en el bloque anterior.

### **Estrategia de Ejecución Eficiente**
Para optimizar el tiempo y los recursos, el código sigue esta lógica:

1.  **Verificación Previa:** Comprueba si la carpeta de audios (`AUDIO_DEV_DIR`) ya existe en Drive. Si es así, **omite** la descarga y descompresión.
2.  **Instalación de Herramientas:** Si es necesario procesar, instala `p7zip-full` y `pigz` para una descompresión multihilo rápida.
3.  **Descarga Temporal:** Descarga el archivo `.zip` en el entorno temporal de Colab (`/content/`) para no desperdiciar almacenamiento en Drive duplicando datos (zip + descomprimido).
4.  **Descompresión Directa:** Descomprime el contenido directamente hacia la ruta final en Drive (`VOX_EXTRACTED_DIR`).

> **Nota:** Este proceso solo se ejecutará completamente la primera vez. En ejecuciones subsiguientes, detectará los archivos y finalizará en segundos.
"""

# ===== BLOQUE 3 — Gestión del Dataset =====

import os
import shutil

# Rutas temporales
VOX_ZIP_TEMP_PATH = "/content/voxconverse_dev.zip"
REPO_TEMP_DIR = "/content/voxconverse_repo"

print("="*40)
print(" GESTIÓN DEL DATASET")
print("="*40)

# --- PARTE 1: AUDIOS ---
# Verificamos si la carpeta tiene archivos .wav
files_in_audio = []
if os.path.exists(AUDIO_DEV_DIR):
    files_in_audio = [f for f in os.listdir(AUDIO_DEV_DIR) if f.endswith('.wav')]

# Lógica simplificada: Si ya hay audios, avisamos. Si no, descargamos.
if len(files_in_audio) > 0:
    print(f"\nAudios detectados: {len(files_in_audio)} archivos encontrados.")

if len(files_in_audio) == 0:
    print("\nAudios no encontrados. Iniciando descarga...")

    # 1. Instalar herramientas
    print("   Instalando herramientas de compresión...")
    !apt-get install -y p7zip-full pigz > /dev/null

    # 2. Descargar ZIP (Solo si no existe el zip)
    if not os.path.exists(VOX_ZIP_TEMP_PATH):
        print("Descargando ZIP de audios...")
        !wget -c --no-check-certificate https://mmai.io/datasets/voxconverse/data/voxconverse_dev_wav.zip -O {VOX_ZIP_TEMP_PATH}

    # 3. Descomprimir
    print(f"Descomprimiendo en: {VOX_EXTRACTED_DIR}")
    !7z x {VOX_ZIP_TEMP_PATH} -o{VOX_EXTRACTED_DIR} -y > /dev/null
    print("Descompresión de audios lista.")


# --- PARTE 2: RTTM (ANNOTATIONS) ---
# Verificamos si existen los RTTM
files_in_rttm = []
if os.path.exists(RTTM_DEV_DIR):
    files_in_rttm = [f for f in os.listdir(RTTM_DEV_DIR) if f.endswith('.rttm')]

if len(files_in_rttm) > 0:
    print(f"RTTMs detectados: {len(files_in_rttm)} archivos encontrados.")

if len(files_in_rttm) == 0:
    print("\nFaltan los archivos RTTM (Ground Truth). Descargando del repo oficial...")

    # Clonamos el repo oficial temporalmente
    if os.path.exists(REPO_TEMP_DIR):
        shutil.rmtree(REPO_TEMP_DIR)

    print("Clonando repositorio...")
    !git clone https://github.com/joonson/voxconverse.git {REPO_TEMP_DIR} > /dev/null

    # Movemos los rttm de dev a nuestra carpeta
    source_rttm = os.path.join(REPO_TEMP_DIR, "dev")
    count = 0

    # Nos aseguramos que la carpeta destino exista
    if not os.path.exists(RTTM_DEV_DIR):
        os.makedirs(RTTM_DEV_DIR)

    if os.path.exists(source_rttm):
        for f in os.listdir(source_rttm):
            if f.endswith(".rttm"):
                shutil.copy(os.path.join(source_rttm, f), RTTM_DEV_DIR)
                count += 1

    print(f"Se han copiado {count} archivos RTTM a: {RTTM_DEV_DIR}")

    # Limpieza
    if os.path.exists(REPO_TEMP_DIR):
        shutil.rmtree(REPO_TEMP_DIR)

print("\nBLOQUE 3 COMPLETADO: Audios y RTTMs listos.")

"""# **BLOQUE 4 — Análisis Exploratorio del Dataset (EDA) EJERCICIO 1**

En este bloque realizamos el análisis estadístico del dataset utilizando **únicamente los archivos RTTM**. Esto es mucho más rápido que procesar los audios.

**Objetivos del análisis:**
- Calcular la duración total del dataset.
- Analizar la distribución de hablantes por archivo.
- Calcular métricas de solapamiento (overlap) y silencios.
- Generar gráficos descriptivos.

Al finalizar, los resultados se guardan en `CKPT_DIR/eda_results_voxconverse.pkl` para no tener que recalcularlos en el futuro.

**Importación de Librerías para el Análisis Exploratorio**

En esta celda importamos las herramientas fundamentales para procesar los metadatos de audio y generar las visualizaciones estadísticas:

* **Gestión de Archivos y Persistencia:**
    * `os`: Para navegar por el sistema de archivos y gestionar las rutas de los datasets.
    * `pickle`: Para guardar y cargar los resultados del análisis ("checkpoints"), evitando tener que reprocesar los datos cada vez que se abre el notebook.

* **Procesamiento de Datos:**
    * `numpy` y `pandas`: Se utilizan para los cálculos numéricos (como duraciones y promedios) y para estructurar la información en un DataFrame tabular fácil de analizar.

* **Visualización:**
    * `matplotlib.pyplot` y `seaborn`: Librerías gráficas para generar los histogramas y gráficos de distribución solicitados en el ejercicio.

* **Diarización (Core):**
    * `pyannote.core`: De aquí importamos `Segment` y `Annotation`, las clases específicas que nos permiten manipular intervalos de tiempo y etiquetas de hablantes extraídas de los archivos RTTM.
"""

# ===== BLOQUE 4 — Análisis Exploratorio (Versión "Fixed & Robust") =====

import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyannote.core import Segment, Annotation

"""**Configuración de Rutas y Utilidades**

En esta sección inicializamos las variables globales que controlan el flujo de datos del Análisis Exploratorio:

1.  **Definición de Directorios:**
    * `ROOT_DIR`, `CKPT_DIR`, `RTTM_DEV_DIR`: Establecen las rutas absolutas a las carpetas de trabajo en Google Drive. Esto asegura que el código sepa exactamente dónde buscar los archivos RTTM y dónde guardar el archivo de resultados (`eda_results_voxconverse.pkl`).

2.  **Gestión de Dependencias (`tqdm`):**
    * Se incluye un bloque de control `try-except` para importar la librería `tqdm`.
    * **Función:** Esta librería es esencial para generar **barras de progreso** visuales durante el bucle de procesamiento de archivos, permitiendo monitorear el avance de la tarea en tiempo real. Si no está instalada en el entorno, el script la instala automáticamente.
"""

# 1. Configuración
ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
CKPT_DIR = f"{ROOT_DIR}/checkpoints"
RTTM_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/rttm/dev"
EDA_CKPT_PATH = f"{CKPT_DIR}/eda_results_voxconverse.pkl"

# Asegurar tqdm
try:
    from tqdm.notebook import tqdm
except ImportError:
    !pip install tqdm
    from tqdm.notebook import tqdm

print("="*40)
print(" INICIANDO ANÁLISIS EDA")
print("="*40)

"""**Definición de Funciones Auxiliares de Procesamiento**

En esta celda definimos dos funciones críticas que automatizan la extracción de información y el cálculo de métricas estadísticas a partir de los archivos `.RTTM`:

1.  **`parse_rttm_file(rttm_path, uri)`**:
    * **Función:** Lee un archivo de texto RTTM línea por línea y lo convierte en un objeto estructurado `Annotation` de la librería *pyannote.core*.
    * **Lógica:** Extrae el tiempo de inicio (`start`), la duración y la etiqueta del hablante. Filtra cualquier segmento con duración 0 o negativa para evitar errores de cálculo.

2.  **`calculate_stats(annotation)`**:
    * **Función:** Recibe el objeto `Annotation` y calcula las 5 métricas clave solicitadas en el análisis exploratorio:
        * **Duración Total:** Tiempo desde el inicio del primer segmento hasta el final del último (`extent`).
        * **Cantidad de Hablantes:** Número de etiquetas únicas encontradas
        * **Promedio de Habla:** Duración media de intervención por hablante.
        * **Silencio:** Calculado como la diferencia entre la duración total del archivo y la suma de los tiempos de habla activa.
        * **Overlap (Superposición):** Tiempo total en el que dos o más hablantes intervienen simultáneamente, utilizando el método nativo `.get_overlap()`.
"""

# 2. Funciones
def parse_rttm_file(rttm_path, uri):
    annotation = Annotation(uri=uri)
    with open(rttm_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 9 and parts[0] == 'SPEAKER':
                start = float(parts[3])
                duration = float(parts[4])
                if duration > 0:
                    annotation[Segment(start, start + duration)] = parts[7]
    return annotation

def calculate_stats(annotation):
    if not annotation: return None

    timeline = annotation.get_timeline()
    labels = annotation.labels()
    n_speakers = len(labels)

    if n_speakers == 0: return 0, 0, 0, 0, 0
    durations = [annotation.label_duration(l) for l in labels]
    avg_speech = np.mean(durations) if durations else 0

    # Overlap
    try:
        overlap_dur = annotation.get_overlap().duration()
    except:
        overlap_dur = 0.0

    return (timeline.extent().duration, n_speakers, avg_speech,
            timeline.extent().duration - timeline.duration(),
            overlap_dur)

"""### **Ejecución del Pipeline de Procesamiento**

En esta celda se orquesta el flujo principal del análisis exploratorio. El código realiza las siguientes operaciones secuenciales:

1.  **Descubrimiento de Archivos:** Escanea el directorio `RTTM_DEV_DIR` para listar todos los archivos de anotación (`.rttm`) disponibles.
2.  **Gestión de Checkpoints:** Antes de procesar, verifica si existe un archivo de resultados previo (`.pkl`).
    * **Validación de Integridad:** Si el archivo existe pero está vacío o corrupto (lo cual puede ocurrir si se interrumpió una ejecución anterior), el código lo elimina automáticamente para forzar un recálculo limpio.
3.  **Iteración y Cálculo:** Recorre cada archivo RTTM utilizando un bucle con `tqdm` (para visualizar el progreso).
    * Por cada archivo, invoca las funciones `parse_rttm_file` y `calculate_stats`.
    * Agrega los resultados validados a una lista `data`, estructurando la información para su posterior conversión a DataFrame.
4.  **Manejo de Errores:** Incluye un bloque `try-except` dentro del bucle para asegurar que si un archivo individual está dañado, el proceso no se detenga y continúe con el resto del dataset.
"""

# 3. Ejecución
print(f"Leyendo archivos desde: {RTTM_DEV_DIR}")
rttm_files = [f for f in os.listdir(RTTM_DEV_DIR) if f.endswith(".rttm")]
print(f"Archivos encontrados: {len(rttm_files)}")

data = []
print("Procesando...")

# Si hay checkpoint previo corrupto, lo borramos para recalcular limpio
if os.path.exists(EDA_CKPT_PATH):
    # Verificamos si es válido cargándolo
    try:
        with open(EDA_CKPT_PATH, "rb") as f:
            test = pickle.load(f)
            if test.empty:
                os.remove(EDA_CKPT_PATH) # Borrar si está vacío
    except:
        os.remove(EDA_CKPT_PATH) # Borrar si da error

for rttm_file in tqdm(rttm_files):
    uri = os.path.splitext(rttm_file)[0]
    path = os.path.join(RTTM_DEV_DIR, rttm_file)

    try:
        ann = parse_rttm_file(path, uri)
        stats = calculate_stats(ann)

        if stats is not None:
            data.append({
                "uri": uri,
                "total_duration_s": stats[0],
                "n_speakers": stats[1],
                "avg_speech_per_speaker_s": stats[2],
                "silence_s": stats[3],
                "overlap_s": stats[4]
            })
    except Exception as e:
        print(f"Error en {uri}: {e}")

"""### **Visualización de Resultados y Estadísticas Descriptivas**

En esta etapa final del EDA, utilizamos las librerías `seaborn` y `matplotlib` para generar una representación gráfica de los metadatos procesados. El objetivo es cumplir con los requerimientos del **Ejercicio 1**, analizando cuatro dimensiones clave del dataset:

1.  **Duración de los Audios:** Visualizamos la distribución temporal para comprender la variabilidad de las grabaciones.
2.  **Cantidad de Hablantes:** Un gráfico de conteo para observar la complejidad conversacional (número de participantes por sesión).
3.  **Distribución de Silencios:** Permite analizar el ratio de actividad vocal frente al tiempo total.
4.  **Superposición (Overlap):** Analizamos los segundos donde los hablantes se interrumpen, lo que indica si las conversaciones son fluidas o entrecortadas.

Adicionalmente, se imprime una **tabla de estadísticas descriptivas** (`describe()`) que resume numéricamente la media, desviación estándar, mínimos y máximos de estas variables.
"""

# 4. Visualización
if not eda_results.empty:
    print("\nGenerando gráficos...")

    # Aumentamos el tamaño para dar espacio (ancho=16, alto=12)
    sns.set_theme(style="whitegrid", context="notebook")
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # Título principal con espacio
    fig.suptitle(f'Análisis Estadístico de VoxConverse (N={len(eda_results)})', fontsize=20, fontweight='bold', y=0.98)

    # Gráfico 1: Duración
    sns.histplot(data=eda_results, x="total_duration_s", bins=30, kde=True, color="#3498db", edgecolor="black", alpha=0.7, ax=axes[0, 0])
    axes[0, 0].set_title("Duración Total de los Audios", fontsize=14, pad=15)
    axes[0, 0].set_xlabel("Segundos", fontsize=12)
    axes[0, 0].set_ylabel("Frecuencia", fontsize=12)

    # Gráfico 2: Hablantes
    sns.countplot(data=eda_results, x="n_speakers", palette="viridis", edgecolor="black", ax=axes[0, 1])
    axes[0, 1].set_title("Distribución de Hablantes por Audio", fontsize=14, pad=15)
    axes[0, 1].set_xlabel("Cantidad de Hablantes", fontsize=12)
    axes[0, 1].set_ylabel("Cantidad de Archivos", fontsize=12)

    # Gráfico 3: Silencios
    sns.histplot(data=eda_results, x="silence_s", bins=30, color="#2ecc71", edgecolor="black", alpha=0.7, ax=axes[1, 0])
    axes[1, 0].set_title("Distribución de Silencios", fontsize=14, pad=15)
    axes[1, 0].set_xlabel("Segundos de Silencio", fontsize=12)
    axes[1, 0].set_ylabel("Frecuencia", fontsize=12)

    # Gráfico 4: Overlap
    sns.histplot(data=eda_results, x="overlap_s", bins=30, color="#e67e22", edgecolor="black", alpha=0.7, ax=axes[1, 1])
    axes[1, 1].set_title("Distribución de Solapamiento (Overlap)", fontsize=14, pad=15)
    axes[1, 1].set_xlabel("Segundos de Overlap", fontsize=12)
    axes[1, 1].set_ylabel("Frecuencia", fontsize=12)

    # --- LA MAGIA: AJUSTAR ESPACIOS ---
    # hspace: espacio vertical, wspace: espacio horizontal
    plt.subplots_adjust(hspace=0.3, wspace=0.2)

    plt.show()

    print("\nEstadísticas descriptivas:")
    display(eda_results.describe().round(2))

else:
    print("ERROR: El DataFrame sigue vacío.")

"""# **BLOQUE 5 — Organización del Dataset en Lotes (Batching) EJERCICIO 2**

Para cumplir con el **Ejercicio 2** y evitar saturar la memoria RAM durante la extracción de características (Bloque 6), no procesaremos los 216 audios de golpe. En su lugar, los dividiremos en **lotes (batches) lógicos**.

### **Objetivos de este bloque:**
1.  **Listar** todos los audios `.wav` disponibles en la carpeta del dataset.
2.  **Agrupar** los identificadores (URIs) en listas de tamaño fijo (por defecto $N=20$).
3.  **Persistir** esta estructura en un archivo `batches_dev.pkl`.

> **Nota:** No se mueven archivos físicamente. Solo creamos una lista de listas que servirá como "mapa de ruta" para el siguiente bloque.

**Importación de Librerías para la Segmentación por Lotes**

En esta celda inicializamos las herramientas necesarias para la lógica de *batching* descrita en el **Ejercicio 2**:

* **`os`**: Necesario para validar la existencia de los directorios del dataset y listar los archivos de audio `.wav`.
* **`pickle`**: Se utiliza para serializar (guardar) la estructura de listas generada en el disco (`batches_dev.pkl`), permitiendo recuperarla rápidamente sin tener que volver a leer la carpeta.
* **`math.ceil`**: Función matemática de "techo" (redondeo hacia arriba). Es fundamental para calcular cuántos lotes se necesitan para cubrir el total de archivos (por ejemplo, si hay 216 archivos y el tamaño del lote es 20, `ceil(216/20)` asegura que se creen 11 lotes, garantizando que los últimos 16 archivos no queden fuera).
"""

# ===== BLOQUE 5 — Organización en Lotes (Batches) =====

import os
import pickle
from math import ceil

"""**Configuración de Rutas y Mecanismo de Seguridad**

Esta celda establece las variables de entorno críticas para la tarea de organización en lotes. Se incluye una lógica de **recuperación automática**:

1.  **Validación de Variables Globales:**
    * El condicional `if 'AUDIO_DEV_DIR' not in globals():` verifica si las rutas principales ya están definidas en la memoria.
    * **Objetivo:** Si el entorno de ejecución se reinició (perdiendo las variables en RAM) pero el disco sigue montado, este bloque redefine las rutas automáticamente para evitar errores de tipo `NameError` sin necesidad de volver a ejecutar los bloques iniciales.

2.  **Definición del Checkpoint:**
    * Se establece `BATCHES_CKPT_PATH`, que indica la ruta exacta donde se guardará el archivo serializado (`.pkl`) con la lista de lotes organizados.
"""

# 1. Recuperación de Rutas (Seguridad ante reinicios)
if 'AUDIO_DEV_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    # Ruta corregida donde viven los .wav
    AUDIO_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/audio"

BATCHES_CKPT_PATH = f"{CKPT_DIR}/batches_dev.pkl"

print("="*40)
print(" ORGANIZACIÓN DE LOTES (BATCHING)")
print("="*40)

"""**Lógica de Creación de Lotes**

En esta celda definimos la función `create_batches`, encargada de transformar la carpeta de archivos en una estructura de datos organizada. El proceso consta de tres pasos clave:

1.  **Validación y Filtrado:** Verifica la existencia del directorio y filtra exclusivamente los archivos con extensión `.wav`, ignorando otros tipos de archivos.
2.  **Normalización y Reproducibilidad:**
    * Elimina la extensión `.wav` para trabajar únicamente con los **URIs** (identificadores únicos) de los audios.
    * Aplica `sorted()` a la lista. Esto es crucial para asegurar la **reproducibilidad**: garantiza que el orden de los archivos sea siempre el mismo en cada ejecución, independientemente de cómo el sistema operativo liste los archivos.
3.  **Paginación (Slicing):** Utiliza operaciones de corte de listas (`slice`) para generar subgrupos de tamaño `batch_size` (20 por defecto), devolviendo una lista de listas listas para ser procesadas.
"""

# 2. Función de Creación de Lotes
def create_batches(audio_dir, batch_size=20):
    # Validación de seguridad
    if not os.path.exists(audio_dir):
        print(f"ERROR CRÍTICO: No existe la carpeta de audios: {audio_dir}")
        print("   Ejecuta el Bloque 3 para descargar el dataset.")
        return []

    # Listar solo archivos .wav y ordenarlos (crucial para reproducibilidad)
    wavs = sorted([f for f in os.listdir(audio_dir) if f.endswith(".wav")])

    if not wavs:
        print("ERROR: La carpeta de audios está vacía.")
        return []

    # Extraer URIs (nombre del archivo sin la extensión .wav)
    uris = [os.path.splitext(w)[0] for w in wavs]
    total_files = len(uris)

    print(f"Total de audios encontrados: {total_files}")

    # Calcular cuántos lotes necesitamos
    num_batches = ceil(total_files / batch_size)
    batch_list = []

    # Dividir la lista principal en sub-listas (slices)
    for i in range(num_batches):
        start = i * batch_size
        end = start + batch_size
        current_batch = uris[start:end]
        batch_list.append(current_batch)

    return batch_list

"""### **Ejecución Lógica con Persistencia de Datos**

En esta sección final del bloque, implementamos la lógica de control de flujo para la gestión de los lotes. El objetivo es maximizar la eficiencia mediante un sistema de **caché en disco**:

1.  **Verificación de Existencia:** Antes de computar, el código consulta si ya existe un archivo de "checkpoint" (`batches_dev.pkl`).
2.  **Recuperación (Carga):** Si el archivo existe, se carga inmediatamente utilizando `pickle`, ahorrando el tiempo de re-escanear el directorio. Se incluye manejo de errores (`try-except`) para regenerar el archivo automáticamente si este estuviera dañado.
3.  **Generación y Persistencia:** Si no existe un checkpoint válido, se invoca a `create_batches` y el resultado se guarda inmediatamente en el disco. Esto asegura que en futuras ejecuciones (o si se reinicia el entorno), el paso se omita.
4.  **Validación Visual:** Finalmente, se imprime un resumen (cantidad de lotes y una muestra del primero) para confirmar visualmente que la estructura de datos es correcta antes de avanzar.
"""

# 3. Ejecución Lógica con Checkpoint
# Verificamos si ya existe el archivo para no repetir trabajo
batches = None

if os.path.exists(BATCHES_CKPT_PATH):
    print("Checkpoint de lotes encontrado. Cargando...")
    try:
        with open(BATCHES_CKPT_PATH, "rb") as f:
            batches = pickle.load(f)
        print(f"Cargados {len(batches)} lotes desde el disco.")
    except Exception as e:
        print(f"El checkpoint estaba dañado. Se regenerará. Error: {e}")
        batches = None

# Si no se cargaron (o no existían), los creamos ahora
if batches is None:
    print(f"Creando lotes de 20 audios desde: {AUDIO_DEV_DIR}")

    batches = create_batches(AUDIO_DEV_DIR, batch_size=20)

    if batches:
        # Guardar checkpoint
        if not os.path.exists(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)

        with open(BATCHES_CKPT_PATH, "wb") as f:
            pickle.dump(batches, f)

        print(f"\nCheckpoint guardado en: {BATCHES_CKPT_PATH}")
        print(f"Total de lotes creados: {len(batches)}")

        # Mostrar estructura del primer lote como ejemplo
        print("\nEjemplo del Lote 1:")
        print(f"   Cantidad de audios: {len(batches[0])}")
        print(f"   Primeros 3 URIs: {batches[0][:3]} ...")
    else:
        print("No se pudieron crear los lotes. Revisa si la carpeta de audio es correcta.")

# Verificación final
if batches:
    print(f"\nBLOQUE 5 COMPLETADO. Listo para procesar el Bloque 6.")

"""# **BLOQUE 6 — Creación del Dataset PyTorch y Segmentación (Ejercicio 2)**

En este bloque preparamos los datos para el entrenamiento.
Como los archivos de audio son largos, necesitamos recortarlos en segmentos pequeños basados en las anotaciones RTTM.

**Proceso:**
1.  **Validación de Checkpoints:** Verifica si los datos guardados son compatibles con la versión actual del código.
2.  **Generación de Segmentos:** Si no hay datos válidos, lee los RTTM y crea la lista de segmentos.
3.  **Dataset Class:** Define la clase `VoxConverseDataset` que convierte audio a Espectrogramas Mel en tiempo real.

**Importación de Librerías de Deep Learning**
Inicializamos los módulos esenciales del ecosistema PyTorch para el procesamiento de audio:

* **`torch`**: El framework principal de Deep Learning que nos permitirá manipular tensores y construir la red neuronal.
* **`torchaudio`**: Librería especializada que utilizaremos para:
    * Cargar los archivos `.wav` en memoria.
    * Realizar transformaciones de señal (como el **MelSpectrogram** solicitado en la consigna).
    * Gestionar el re-muestreo (resampling) si fuera necesario.
* **`Dataset` y `DataLoader`**: Clases fundamentales de PyTorch.
    * **`Dataset`**: Nos permite definir la lógica personalizada para extraer un *solo* segmento de audio a partir de la lista de metadatos.
    * **`DataLoader`**: Se encarga de agrupar esos segmentos en lotes (batches), mezclarlos (`shuffle`) y prepararlos para el entrenamiento.
"""

# ===== BLOQUE 6 — Dataset PyTorch y Segmentación =====

import os
import pickle
import torch
import torchaudio
from torch.utils.data import Dataset, DataLoader
from tqdm.notebook import tqdm

"""**Configuración de Rutas y Archivos de Salida**

Esta celda establece la estructura de directorios necesaria para la etapa de segmentación. Se destacan dos aspectos clave para la robustez del pipeline:

1.  **Mecanismo de Recuperación:** Se incluye una validación condicional (`if ... not in globals`) para redefinir las rutas maestras si estas se perdieron de la memoria RAM por un reinicio del entorno. Esto garantiza que el bloque sea ejecutable de forma independiente.

2.  **Definición de Checkpoints (Salidas):** Se especifican las rutas exactas donde se persistirán los resultados de este proceso:
    * **`segments_dev.pkl`**: Archivo que almacenará la lista maestra de metadatos (ruta del audio, inicio, fin y ID del hablante) para cada segmento generado.
    * **`speakers_dev.pkl`**: Archivo que guardará el diccionario de mapeo, traduciendo los nombres de los hablantes (cadenas de texto como `spk00`) a índices numéricos (enteros como `0`) requeridos por la red neuronal.
"""

# 1. Rutas
if 'AUDIO_DEV_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    AUDIO_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/audio"
    RTTM_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/rttm/dev"

SEGMENTS_CKPT = f"{CKPT_DIR}/segments_dev.pkl"
SPEAKERMAP_CKPT = f"{CKPT_DIR}/speakers_dev.pkl"

print("="*40)
print("  GENERACIÓN DE SEGMENTOS")
print("="*40)

"""**Función de Lectura y Preprocesamiento de RTTM**

En esta celda definimos `load_rttm_segments`, una función crítica para la ingestión de datos etiquetados. Su objetivo es transformar el texto crudo de los archivos `.rttm` en una lista estructurada de segmentos de audio.

**Detalles de la implementación:**
1.  **Parsing Estándar:** Lee cada línea buscando la etiqueta `SPEAKER`, extrayendo el tiempo de inicio (`start`) y la duración.
2.  **Cálculo de Intervalos:** Convierte la duración en un tiempo de finalización (`end = start + duration`), necesario para recortar el audio posteriormente.
3.  **Filtrado de Calidad:** Implementa un filtro condicional (`if duration > 0.5`) para descartar micro-segmentos menores a 500ms.
    
    
"""

# 2. Función de lectura RTTM
def load_rttm_segments(rttm_path):
    segments = []
    with open(rttm_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 9 and parts[0] == 'SPEAKER':
                start = float(parts[3])
                duration = float(parts[4])
                # Filtro de duración mínima (0.5s)
                if duration > 0.5:
                    segments.append((start, start + duration, parts[7]))
    return segments

"""**Validación de Integridad y Compatibilidad de Checkpoints**

En esta sección implementamos una lógica de **"Auto-Reparación"** para la carga de datos. Dado que el proceso de desarrollo es iterativo, es posible que los archivos guardados en ejecuciones anteriores tengan una estructura obsoleta.

El código realiza las siguientes verificaciones:
1.  **Existencia:** Confirma que los archivos `.pkl` existan en el disco.
2.  **Compatibilidad de Esquema:** Verifica si los datos cargados contienen la clave **`speaker_idx`**.
    
"""

# 3. Lógica de Carga y Validación de Versión
regenerar = True

if os.path.exists(SEGMENTS_CKPT) and os.path.exists(SPEAKERMAP_CKPT):
    print(" Checkpoints encontrados. Validando compatibilidad...")
    try:
        with open(SEGMENTS_CKPT, "rb") as f:
            segments_list = pickle.load(f)
        with open(SPEAKERMAP_CKPT, "rb") as f:
            speaker_to_id = pickle.load(f)

        # VALIDACIÓN CRÍTICA: ¿El archivo viejo tiene las claves nuevas?
        if len(segments_list) > 0:
            # Si falta la clave 'speaker_idx', es un archivo viejo
            if "speaker_idx" in segments_list[0]:
                print(" Checkpoint compatible. Usando datos guardados.")
                regenerar = False
            else:
                print(" Checkpoint obsoleto (formato antiguo). Se regenerará.")
        else:
            print(" Checkpoint vacío. Se regenerará.")

    except Exception as e:
        print(f" Error leyendo checkpoint ({e}). Se regenerará.")
        regenerar = True

"""**Generación de la Lista Maestra de Segmentos**

Si la validación anterior determina que es necesario regenerar los datos (variable `regenerar`), se ejecuta este bloque lógico:

1.  **Iteración sobre el Dataset:** Recorre cada archivo de audio `.wav` y busca su correspondiente anotación `.rttm`.
2.  **Mapeo de Hablantes (Label Encoding):**
    * Las redes neuronales requieren etiquetas numéricas para calcular la función de pérdida (*CrossEntropy*).
    * Se crea un diccionario `speaker_to_id` que asigna un entero único a cada hablante nuevo que aparece (ej: `spk00` $\rightarrow$ `0`, `spk01` $\rightarrow$ `1`).
3.  **Construcción de Metadatos:** Se genera una lista de diccionarios (`segments_list`) donde cada elemento representa un fragmento de audio listo para ser procesado.
4.  **Persistencia:** Finalmente, se guardan las estructuras procesadas en disco (`.pkl`), completando el preprocesamiento del **Ejercicio 2**.
"""

# 4. Generación de Segmentos (si es necesario)
if regenerar:
    print("Generando lista de segmentos desde cero...")
    segments_list = []
    speaker_to_id = {}
    speaker_counter = 0

    wav_files = sorted([f for f in os.listdir(AUDIO_DEV_DIR) if f.endswith(".wav")])

    for wav_name in tqdm(wav_files, desc="Audios"):
        uri = os.path.splitext(wav_name)[0]
        rttm_path = os.path.join(RTTM_DEV_DIR, f"{uri}.rttm")
        wav_path = os.path.join(AUDIO_DEV_DIR, wav_name)

        if not os.path.exists(rttm_path): continue

        segs = load_rttm_segments(rttm_path)

        for start, end, spk_label in segs:
            if spk_label not in speaker_to_id:
                speaker_to_id[spk_label] = speaker_counter
                speaker_counter += 1

            # AQUI ESTABA EL CAMBIO CLAVE: "speaker_idx"
            segments_list.append({
                "wav_path": wav_path,
                "start": start,
                "end": end,
                "speaker_idx": speaker_to_id[spk_label]
            })

    # Guardado
    if not os.path.exists(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)
    with open(SEGMENTS_CKPT, "wb") as f: pickle.dump(segments_list, f)
    with open(SPEAKERMAP_CKPT, "wb") as f: pickle.dump(speaker_to_id, f)
    print(f"Guardado: {len(segments_list)} segmentos, {len(speaker_to_id)} hablantes.")

"""### **Definición de la Clase VoxConverseDataset**

En esta celda implementamos la clase personalizada `VoxConverseDataset`, heredando de `torch.utils.data.Dataset`. Esta clase es el motor del pipeline de datos.

**Funcionalidades Clave:**

1.  **Transformación de Audio:** En el constructor (`__init__`), definimos las transformaciones necesarias para convertir el audio crudo (Waveform) en un **MelSpectrogram**. Esto permite que el modelo procese el sonido como si fuera una imagen.
2.  **Carga y Recorte (`__getitem__`):**
    * Carga el archivo de audio completo utilizando `torchaudio`.
    * Realiza el recorte (*slicing*) preciso del segmento correspondiente al hablante, basándose en los tiempos de inicio y fin calculados previamente.
    * Asegura que la frecuencia de muestreo sea siempre **16kHz** (re-muestreo si es necesario).
3.  **Robustez:** Implementa bloques de seguridad (`try-except` y validación de límites) para evitar que el entrenamiento se detenga si se encuentra un archivo de audio corrupto o un segmento con duración inválida, devolviendo tensores de silencio en esos casos excepcionales.
4.  **Salida:** Retorna una tupla `(tensor_espectrograma, speaker_id)`, lista para ser consumida por el modelo.
"""

# 5. Dataset Class
class VoxConverseDataset(Dataset):
    def __init__(self, segments, sample_rate=16000, n_mels=64):
        self.segments = segments
        self.sample_rate = sample_rate
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate, n_fft=512, hop_length=160, n_mels=n_mels
        )
        self.db_transform = torchaudio.transforms.AmplitudeToDB()

    def __len__(self): return len(self.segments)

    def __getitem__(self, idx):
        item = self.segments[idx]
        # Carga robusta
        try:
            waveform, sr = torchaudio.load(item["wav_path"])
        except Exception:
            # Si falla la carga, devolvemos silencio (evita crash total)
            return torch.zeros(1, 64, 100), torch.tensor(item["speaker_idx"])

        if sr != self.sample_rate:
            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)

        start_sample = int(item["start"] * self.sample_rate)
        end_sample = int(item["end"] * self.sample_rate)

        # Padding seguro
        if end_sample > waveform.shape[1]: end_sample = waveform.shape[1]

        # Si el segmento es inválido o muy corto
        if start_sample >= end_sample:
             return torch.zeros(1, 64, 10), torch.tensor(item["speaker_idx"])

        segment_wave = waveform[:, start_sample:end_sample]
        mel_db = self.db_transform(self.mel_transform(segment_wave))
        return mel_db, item["speaker_idx"]

"""**Función de Collate para Padding Dinámico**

En esta celda implemento `collate_fn`, una función auxiliar crítica que el `DataLoader` utiliza para ensamblar los lotes de datos.

Es necesaria porque los segmentos de audio tienen duraciones variables (algunos duran 2 segundos, otros 5), lo que genera espectrogramas de distinto ancho (tiempo). Las redes neuronales requieren que todos los tensores de un mismo lote tengan dimensiones idénticas.

**Proceso que realiza:**
1.  **Filtrado:** Descarta cualquier muestra vacía o corrupta que haya fallado en la carga previa.
2.  **Padding Dinámico:** Utiliza `pad_sequence` para rellenar con ceros los espectrogramas más cortos hasta alcanzar la longitud del más largo del lote.
3.  **Reordenamiento de Dimensiones:** Ajusta el tensor final a la forma `(Batch, 1, Mel, Time)` requerida por la arquitectura convolucional (ResNet), donde `1` representa el canal (monocanal).

"""

# 6. Collate Function
def collate_fn(batch):
    # Filtrar tensores vacíos o erróneos
    batch = [b for b in batch if b[0].shape[-1] > 0]
    if not batch: return torch.tensor([]), torch.tensor([]), None

    specs, speakers = zip(*batch)
    # Permutar para pad_sequence: (Time, Mel)
    specs_permuted = [s.squeeze(0).transpose(0, 1) for s in specs]
    padded = torch.nn.utils.rnn.pad_sequence(specs_permuted, batch_first=True)
    # Volver a (Batch, 1, Mel, Time)
    padded = padded.transpose(1, 2).unsqueeze(1)
    return padded, torch.tensor(speakers), None

"""### **Prueba**

Antes de proceder al diseño del modelo, realizo una **prueba unitaria** para validar que el `Dataset` y el `DataLoader` funcionan correctamente en conjunto.

**Objetivos de la prueba:**
1.  **Instanciación:** Crear un mini-dataset con solo 4 muestras para verificar la carga rápida.
2.  **Batching:** Procesar un lote pequeño (`batch_size=2`) utilizando la función `collate_fn`.
3.  **Verificación Dimensional:** Confirmar que los tensores resultantes tengan la forma correcta `(Batch, Channel, Mel, Time)` que espera la red neuronal.
    * Si el output muestra un tensor válido (ej: `[2, 1, 64, ...]`), confirmamos que el pipeline de datos está listo para la etapa de entrenamiento.
"""

# 7. Prueba
print("\n Testeando Dataset...")
try:
    if len(segments_list) > 0:
        ds = VoxConverseDataset(segments_list[:4])
        dl = DataLoader(ds, batch_size=2, collate_fn=collate_fn)
        s, sp, _ = next(iter(dl))
        print(f" Batch OK. Shape: {s.shape}")
    else:
        print(" No hay segmentos para probar.")
except Exception as e:
    print(f" Error en test: {e}")

"""# **BLOQUE 7 — Definición del Modelo (ResNet50)EJERCICIO 3**

En este bloque se define la arquitectura de la red neuronal que aprenderá a distinguir voces.
Utilizo una **ResNet50**, una arquitectura de convolución profunda muy efectiva para extraer características visuales.

1.  **Entrada (Input):** Las ResNet clásicas esperan imágenes RGB (3 canales). Nuestros espectrogramas son de **1 canal** (escala de grises). Modificaremos la primera capa convolucional (`conv1`) para aceptar `in_channels=1`.
2.  **Salida (Output):** Eliminamos la capa de clasificación original (ImageNet, 1000 clases) y la reemplazamos por una capa lineal que proyecte las características a un **vector de embedding de 512 dimensiones**.

Este vector será la "huella digital" de la voz.

### **Importación de Librerías para la Definición del Modelo**

En esta celda importamos los módulos de *PyTorch* necesarios para construir la red neuronal:

* **`os`**: Para gestionar las rutas de archivos y verificar la existencia de los checkpoints del modelo.
* **`torch`**: El núcleo del framework, necesario para manejar tensores y operaciones en GPU.
* **`torch.nn`**: Provee las clases base para construir redes neuronales, como `nn.Module` (la clase padre de nuestro modelo), `nn.Conv2d` (capas convolucionales) y `nn.Linear` (capas lineales).
* **`torchvision.models`**: Nos da acceso directo a arquitecturas de visión pre-entrenadas, permitiéndonos descargar la **ResNet50** con pesos de *ImageNet* para aplicar *Transfer Learning*.
"""

# ===== BLOQUE 7 — Arquitectura del Modelo (Versión Auto-Reparable) =====

import os
import torch
import torch.nn as nn
import torchvision.models as models

"""### **Configuración de Rutas y Archivo de Checkpoint**

Esta sección establece las variables de entorno necesarias para la gestión del modelo en el disco:

1.  **Recuperación de Variables:**
    * El bloque condicional (`if ... not in globals`) actúa como un mecanismo de seguridad. Si el entorno se reinicia y se pierden las variables en memoria, este código restablece las rutas a Google Drive automáticamente, permitiendo ejecutar este bloque de forma independiente.

2.  **Definición del Checkpoint Inicial:**
    * Se define la variable `MODEL_INIT_CKPT`, que apunta a la ubicación exacta (`model_init.pt`) donde se guardarán los pesos iniciales de la red. Esto es fundamental para verificar posteriormente si ya existe un modelo creado o si se debe instanciar uno desde cero.
"""

# 1. Rutas
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"

MODEL_INIT_CKPT = f"{CKPT_DIR}/model_init.pt"

print("="*40)
print("  DEFINICIÓN DEL MODELO (ResNet50)")
print("="*40)

"""### **Implementación de la Clase SpeakerEmbeddingModel**

En esta celda defino la arquitectura de la red neuronal personalizada, heredando de `nn.Module`.

1.  **Base Pre-entrenada:** Se carga una **ResNet50** con pesos de *ImageNet* (`weights=DEFAULT`). Esto aprovecha el aprendizaje por transferencia, permitiendo que el modelo extraiga patrones complejos desde el inicio.

2.  **Adaptación de Entrada (Capa `conv1`):**
    * La ResNet original espera 3 canales (RGB).
    * Se reemplaza la primera convolución por una nueva `nn.Conv2d` con `in_channels=1`. Esto permite procesar nuestros espectrogramas monocromáticos sin errores de dimensiones.

3.  **Adaptación de Salida (Capa `fc`):**
    * Se elimina el clasificador original de 1000 clases.
    * Se sustituye por una secuencia de capas (`Sequential`) diseñada para generar embeddings:
        * **`Linear`**: Proyecta las características a un vector de tamaño 512.
        * **`BatchNorm1d`**: Normaliza los vectores para estabilizar el entrenamiento.
        * **`ReLU` y `Dropout`**: Añaden no-linealidad y regularización para prevenir el sobreajuste.

4.  **Forward Pass:** El método `forward` simplemente pasa el tensor de entrada a través de esta arquitectura modificada, devolviendo el vector de características (embedding) resultante.
"""

# 2. Clase del Modelo
class SpeakerEmbeddingModel(nn.Module):
    def __init__(self, embedding_dim=512, pretrained=True):
        super().__init__()

        # Cargar ResNet50 pre-entrenada en ImageNet
        weights = models.ResNet50_Weights.DEFAULT if pretrained else None
        self.base = models.resnet50(weights=weights)

        # --- ADAPTACIÓN 1: CANALES DE ENTRADA ---
        self.base.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=64,
            kernel_size=7,
            stride=2,
            padding=3,
            bias=False
        )

        # --- ADAPTACIÓN 2: CAPA DE SALIDA (EMBEDDING) ---
        out_features = self.base.fc.in_features

        self.base.fc = nn.Sequential(
            nn.Linear(out_features, embedding_dim),
            nn.BatchNorm1d(embedding_dim), # Capa que faltaba en tu checkpoint viejo
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def forward(self, x):
        return self.base(x)

"""**Inicialización del Modelo y Gestión de Compatibilidad**

En esta celda instanciamos el modelo y gestionamos su persistencia en el disco con una lógica de **"Auto-Reparación"**:

1.  **Selección de Hardware:**
    * Detecta automáticamente si hay una GPU disponible (`cuda`) para acelerar el entrenamiento. Si no, utiliza la CPU (`cpu`) como respaldo.
    * Mueve el modelo al dispositivo seleccionado mediante `.to(device)`.

2.  **Carga Robusta de Pesos (Checkpointing):**
    * Verifica si ya existe un archivo de pesos iniciales (`model_init.pt`).
    * **Manejo de Conflictos:** Utiliza un bloque `try-except` para intentar cargar los pesos.
        * Si la arquitectura del modelo guardado no coincide con la definición actual (por ejemplo, si agregamos *BatchNorm* recientemente pero el archivo guardado no lo tiene), *PyTorch* lanzará un `RuntimeError`.
        * **Solución Automática:** El código captura este error, elimina el archivo obsoleto y guarda inmediatamente el nuevo estado del modelo. Esto evita bloqueos manuales y asegura que siempre se trabaje con una versión válida.
"""

# 3. Inicialización Inteligente
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Dispositivo seleccionado: {device.upper()}")

# Instanciamos el modelo NUEVO
model = SpeakerEmbeddingModel(embedding_dim=512).to(device)

# Lógica de carga robusta
if os.path.exists(MODEL_INIT_CKPT):
    print(" Checkpoint encontrado. Verificando compatibilidad...")
    try:
        # Intentamos cargar. Si las capas no coinciden, fallará aquí.
        model.load_state_dict(torch.load(MODEL_INIT_CKPT, map_location=device))
        print(" Modelo inicial cargado correctamente.")
    except RuntimeError as e:
        print(" CONFLICTO DETECTADO: El modelo guardado tiene una arquitectura vieja.")
        print(f"   Error: {e}")
        print(" Eliminando checkpoint obsoleto y regenerando uno nuevo...")

        # Borramos el archivo viejo
        os.remove(MODEL_INIT_CKPT)

        # Guardamos el modelo nuevo recién instanciado
        if not os.path.exists(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)
        torch.save(model.state_dict(), MODEL_INIT_CKPT)
        print(" Nuevo modelo inicial guardado y listo.")
else:
    print(" Guardando estado inicial del modelo...")
    if not os.path.exists(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)
    torch.save(model.state_dict(), MODEL_INIT_CKPT)
    print(" Modelo inicial creado.")

print("\n Bloque 7 completado. El modelo está listo para entrenar.")

"""# **BLOQUE 8 — Entrenamiento del Modelo (Training Loop)**

En este bloque ejecutamos el ciclo de entrenamiento.

### **Características del Entrenamiento:**
1.  **Continuidad:** El código verifica si existe un estado previo (`training_state.pt`). Si se corta la luz o se desconecta Colab, al volver a ejecutar este bloque, el entrenamiento retomará desde el último *epoch* completado.
2.  **Batch Size:** Hemos ajustado el tamaño del lote a **16** (u 8 si hay poca memoria) para estabilizar las capas de *Batch Normalization*.
3.  **Guardado Automático:**
    - `model_best.pt`: Se actualiza solo si el *loss* (error) baja.
    - `model_last.pt`: Se guarda al finalizar cada epoch.
    - `training_state.pt`: Guarda el optimizador y el epoch actual.

### **Hiperparámetros:**
- **Epochs:** 3 (Para pruebas iniciales).
- **Learning Rate:** 1e-4.
- **Optimizador:** Adam.

**Importación de Librerías para el Entrenamiento**

En esta celda inicializamos las herramientas necesarias para ejecutar el ciclo de aprendizaje profundo (*Deep Learning*), incluyendo módulos específicos para la optimización de recursos en GPU:

* **Core de PyTorch:**
    * `torch`, `nn`, `optim`: Los componentes fundamentales para definir el grafo computacional, las funciones de pérdida y los algoritmos de optimización (como *Adam*).
    * `DataLoader`: Para la carga eficiente de datos en paralelo.

* **Optimización de Memoria y Velocidad (AMP):**
    * `torch.cuda.amp.autocast` y `GradScaler`: Habilitan el entrenamiento con **Precisión Mixta Automática** (Automatic Mixed Precision). Esto permite que ciertas operaciones se realicen en `float16` en lugar de `float32`, reduciendo drásticamente el uso de memoria VRAM y acelerando el entrenamiento sin perder estabilidad numérica.

* **Gestión de Recursos:**
    * `gc` (Garbage Collector): Se importa explícitamente para forzar la limpieza de la memoria RAM y VRAM entre épocas, previniendo fugas de memoria que podrían detener el entrenamiento en sesiones largas.

* **Visualización:**
    * `matplotlib.pyplot`: Para graficar la curva de pérdida (*loss*) al final del proceso.
    * `tqdm`: Para mostrar barras de progreso interactivas durante las épocas.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
import gc # Garbage Collector para limpiar RAM

"""

---
### **1. Limpieza Preventiva de Recursos**

Antes de iniciar el proceso de carga de datos y configuración del entrenamiento, se ejecuta una limpieza explícita de la memoria para asegurar que la GPU esté lo más vacía posible:

* **`torch.cuda.empty_cache()`**: Libera toda la memoria VRAM que PyTorch haya reservado pero que no esté utilizando actualmente. Esto es crucial porque PyTorch tiende a "acaparar" memoria para acelerar futuras asignaciones, lo que podría causar un falso error de *Out Of Memory*.
* **`gc.collect()`**: Invoca al recolector de basura de Python para eliminar objetos no referenciados de la memoria RAM (CPU), asegurando que el sistema operativo tenga recursos suficientes para los *DataLoaders*.
"""

# 1. Limpieza de Memoria Preventiva
torch.cuda.empty_cache()
gc.collect()

"""

---


### **2.Configuración de Rutas, Archivos de Salida y Hardware**

En esta sección se establecen los parámetros de infraestructura necesarios para el entrenamiento:

1.  **Recuperación de Rutas:**
    * Al igual que en bloques anteriores, se incluye una validación de seguridad (`if ... not in globals`) para redefinir las rutas a los datos (`SEGMENTS_CKPT`, `SPEAKERMAP_CKPT`) si el entorno se ha reiniciado recientemente.

2.  **Definición de Archivos de Checkpoint:**
    * Se definen tres rutas de salida críticas para la persistencia del entrenamiento:
        * **`MODEL_BEST.pt`**: Almacenará los pesos del modelo que logre el **menor error (loss)** histórico. Este es el modelo que usaremos para la evaluación final.
        * **`MODEL_LAST.pt`**: Se sobrescribe al final de cada época con el estado más reciente, sirviendo como copia de seguridad inmediata.
        * **`TRAINING_STATE.pt`**: Guarda metadatos completos (número de época, estado del optimizador, historial de loss) para permitir la **reanudación exacta** del entrenamiento ante interrupciones.

3.  **Selección de Dispositivo:**
    * Se configura el dispositivo de cómputo (`device`). El código prioriza **`cuda`** (GPU) para acelerar el entrenamiento masivo de la red convolucional. Si no detecta una GPU, hace *fallback* a la CPU."""

# 2. Configuración
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    SEGMENTS_CKPT = f"{CKPT_DIR}/segments_dev.pkl"
    SPEAKERMAP_CKPT = f"{CKPT_DIR}/speakers_dev.pkl"
    MODEL_INIT_CKPT = f"{CKPT_DIR}/model_init.pt"

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
MODEL_LAST = f"{CKPT_DIR}/model_last.pt"
TRAINING_STATE = f"{CKPT_DIR}/training_state.pt"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Entrenando en: {device.upper()}")

"""

---


### **3.Carga de Metadatos y Configuración del DataLoader**

En esta sección inicializamos el flujo de datos que alimentará a la GPU. Se destacan dos decisiones de diseño críticas:

1.  **Recuperación de Estructuras Procesadas:**
    * Se carga desde el disco las listas `segments_list` y el diccionario `speaker_to_id` generados en el Bloque 6.
    * Se calcula dinámicamente `num_speakers` basándose en el diccionario cargado. Esto permite que la capa final del modelo se ajuste automáticamente a la cantidad real de hablantes encontrados, sin necesidad de "hardcodear" el número.

2.  **Ajuste de Hiperparámetros (Batch Size):**
    * Se define `BATCH_SIZE = 8`.
    * **Justificación:** Aunque teóricamente un lote mayor (16 o 32) estabiliza mejor los gradientes, las pruebas empíricas demostraron que, debido a la longitud variable de los audios y el padding dinámico, lotes mayores provocaban un desbordamiento de memoria en la GPU T4 (*CUDA Out Of Memory*). Reducir el batch a 8 fue la solución de compromiso para garantizar la ejecución ininterrumpida.

3.  **Instanciación del DataLoader:**
    * Se crea el objeto `DataLoader` con `shuffle=True` (esencial para el entrenamiento estocástico) y se pasa nuestra función personalizada `collate_fn` para manejar el relleno (padding) de los tensores."""

# 3. Carga de Datos
print(" Cargando metadatos...")
try:
    with open(SEGMENTS_CKPT, "rb") as f:
        segments_list = pickle.load(f)
    with open(SPEAKERMAP_CKPT, "rb") as f:
        speaker_to_id = pickle.load(f)

    num_speakers = len(speaker_to_id)

    # --- CAMBIO IMPORTANTE: Reducimos Batch Size a 8 para evitar OOM ---
    BATCH_SIZE = 8
    print(f"   -> Batch Size ajustado a: {BATCH_SIZE}")

    dataset = VoxConverseDataset(segments_list)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

except Exception as e:
    print(f" Error cargando datos: {e}")
    raise

"""

---
4. ### **Instanciación del Modelo y Configuración del Aprendizaje**

En esta sección se definen los componentes matemáticos que guiarán el proceso de aprendizaje:

1.  **Modelo y Clasificador:**
    * Instanciamos la red principal `SpeakerEmbeddingModel` (nuestra ResNet modificada) que genera vectores de 512 dimensiones.
    * **Capa de Clasificación:** Creamos una capa lineal adicional (`classifier`) que conecta los embeddings (512) con el número total de hablantes únicos (`num_speakers`).

2.  **Optimizador (Adam):**
    * Se utiliza el algoritmo **Adam** con una tasa de aprendizaje (*learning rate*) de `1e-4`.
    * **Importante:** Se optimizan conjuntamente los parámetros del modelo (extractor de características) y del clasificador.

3.  **Función de Pérdida:**
    * `CrossEntropyLoss`: La función estándar para clasificación multiclase.

4.  **GradScaler:**
    * Se inicializa el escalador de gradientes necesario para el entrenamiento con **Precisión Mixta**. Esto gestiona automáticamente la escala de los valores numéricos para evitar problemas de desbordamiento (underflow/overflow) al trabajar con `float16`.
"""

# 4. Preparación del Modelo
model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
classifier = nn.Linear(512, num_speakers).to(device)
optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=1e-4)
criterion = nn.CrossEntropyLoss()
scaler = GradScaler()

"""

---
**5. Lógica de Reanudación y Continuidad del Entrenamiento**

En esta sección se implementa el mecanismo de **persistencia del estado**, diseñado para recuperar el trabajo en caso de interrupciones (desconexión de Colab, cortes de energía, etc.):

1.  **Detección de Estado Previo (`TRAINING_STATE`):**
    * El código verifica si existe el archivo `training_state.pt`.
    * **Si existe:** Carga el diccionario completo de entrenamiento, restaurando no solo los pesos del modelo (`model_state`), sino también:
        * El estado del clasificador.
        * El estado interno del optimizador (momentum, promedios móviles).
        * El número de época (`start_epoch`) donde se detuvo.
        * El mejor *loss* histórico (`best_loss`) para seguir comparando correctamente.
    * **Resultado:** El entrenamiento continúa exactamente donde se dejó, sin perder progreso.

2.  **Inicio desde Cero (`MODEL_INIT_CKPT`):**
    * Si no hay un entrenamiento en curso, verifica si existe un modelo inicial (`model_init.pt`) generado en el Bloque 7.
    * Esto asegura que, incluso al empezar de cero, se utilicen los pesos iniciales predefinidos para garantizar la reproducibilidad de los experimentos.
"""

# 5. Lógica de Reanudación
start_epoch = 0
best_loss = float("inf")
loss_history = []

if os.path.exists(TRAINING_STATE):
    print(" Reanudando estado previo...")
    try:
        checkpoint = torch.load(TRAINING_STATE, map_location=device)
        model.load_state_dict(checkpoint['model_state'])
        classifier.load_state_dict(checkpoint['classifier_state'])
        optimizer.load_state_dict(checkpoint['optimizer_state'])
        start_epoch = checkpoint['epoch']
        best_loss = checkpoint['best_loss']
        if 'loss_history' in checkpoint: loss_history = checkpoint['loss_history']
        print(f"   -> Epoch inicial: {start_epoch + 1}")
    except:
        print(" Estado corrupto. Iniciando desde cero.")
elif os.path.exists(MODEL_INIT_CKPT):
    model.load_state_dict(torch.load(MODEL_INIT_CKPT, map_location=device))
    print(" Iniciando desde cero.")

"""

---
### **6 .Ejecución del Ciclo de Entrenamiento (Training Loop)**

En esta sección final del bloque, se lleva a cabo el proceso iterativo de aprendizaje supervisado. El código orquesta las siguientes acciones por cada época (`EPOCHS`):

1.  **Forward Pass (Predicción):**
    * Los datos se mueven a la GPU.
    * El modelo genera embeddings y el clasificador predice a qué hablante pertenecen.
    * Se calcula el error (`loss`) comparando la predicción con la etiqueta real.

2.  **Backward Pass (Optimización):**
    * Se calculan los gradientes (la dirección en la que deben cambiar los pesos para reducir el error).
    * El optimizador actualiza los parámetros de la red.
    * **Nota Técnica:** Se utiliza `scaler` para gestionar la *Precisión Mixta*, evitando problemas numéricos con `float16`.

3.  **Monitoreo y Limpieza:**
    * Se actualiza la barra de progreso con el valor de pérdida actual.
    * Se ejecuta una limpieza periódica de memoria (cada 100 lotes) para prevenir saturación de VRAM.

4.  **Cierre de Época:**
    * Se calcula el error promedio.
    * Si el error es el más bajo hasta la fecha, se guarda el modelo como `model_best.pt`.
    * Siempre se guarda el estado completo (`training_state.pt`) para permitir reanudaciones futuras.

Finalmente, se genera un **gráfico de la curva de pérdida** para visualizar si el modelo está aprendiendo correctamente (la curva debería descender).
"""

# 6. Loop de Entrenamiento
EPOCHS = 3

print("\n" + "="*40)
print(f"  INICIANDO TRAINING (Epochs: {EPOCHS})")
print("="*40)

model.train()
classifier.train()

for epoch in range(start_epoch, start_epoch + EPOCHS):
    running_loss = 0.0
    progress_bar = tqdm(loader, desc=f"Epoch {epoch+1}", leave=True)

    for batch_idx, (specs, speakers, _) in enumerate(progress_bar):
        specs = specs.to(device).float()
        speakers = speakers.to(device).long()

        optimizer.zero_grad()

        # Mixed Precision
        with torch.amp.autocast('cuda', enabled=(device=="cuda")):
            embeddings = model(specs)
            logits = classifier(embeddings)
            loss = criterion(logits, speakers)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        current_loss = loss.item()
        running_loss += current_loss

        if batch_idx % 10 == 0:
            progress_bar.set_postfix({'loss': f"{current_loss:.4f}"})

        # Limpieza periódica de memoria (opcional pero útil)
        if batch_idx % 100 == 0:
            del specs, speakers, embeddings, logits, loss
            torch.cuda.empty_cache()

    # Fin de Epoch
    epoch_avg_loss = running_loss / len(loader)
    loss_history.append(epoch_avg_loss)

    print(f" Epoch {epoch+1} Finalizado | Loss: {epoch_avg_loss:.4f}")

    if epoch_avg_loss < best_loss:
        best_loss = epoch_avg_loss
        torch.save(model.state_dict(), MODEL_BEST)
        print("    Nuevo mejor modelo guardado.")

    state = {
        'epoch': epoch + 1,
        'model_state': model.state_dict(),
        'classifier_state': classifier.state_dict(),
        'optimizer_state': optimizer.state_dict(),
        'best_loss': best_loss,
        'loss_history': loss_history
    }
    torch.save(state, TRAINING_STATE)
    torch.save(model.state_dict(), MODEL_LAST)

print("\n Entrenamiento completado.")

if len(loss_history) > 0:
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o')
    plt.title("Training Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

# ===== BLOQUE 8 — Entrenamiento con Gestión de Memoria =====

import os
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
import gc # Garbage Collector para limpiar RAM

# 1. Limpieza de Memoria Preventiva
torch.cuda.empty_cache()
gc.collect()

# 2. Configuración
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    SEGMENTS_CKPT = f"{CKPT_DIR}/segments_dev.pkl"
    SPEAKERMAP_CKPT = f"{CKPT_DIR}/speakers_dev.pkl"
    MODEL_INIT_CKPT = f"{CKPT_DIR}/model_init.pt"

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
MODEL_LAST = f"{CKPT_DIR}/model_last.pt"
TRAINING_STATE = f"{CKPT_DIR}/training_state.pt"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Entrenando en: {device.upper()}")

# 3. Carga de Datos
print(" Cargando metadatos...")
try:
    with open(SEGMENTS_CKPT, "rb") as f:
        segments_list = pickle.load(f)
    with open(SPEAKERMAP_CKPT, "rb") as f:
        speaker_to_id = pickle.load(f)

    num_speakers = len(speaker_to_id)

    # --- CAMBIO IMPORTANTE: Reducimos Batch Size a 8 para evitar OOM ---
    BATCH_SIZE = 8
    print(f"   -> Batch Size ajustado a: {BATCH_SIZE}")

    dataset = VoxConverseDataset(segments_list)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

except Exception as e:
    print(f" Error cargando datos: {e}")
    raise

# 4. Preparación del Modelo
model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
classifier = nn.Linear(512, num_speakers).to(device)
optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=1e-4)
criterion = nn.CrossEntropyLoss()
scaler = GradScaler()

# 5. Lógica de Reanudación
start_epoch = 0
best_loss = float("inf")
loss_history = []

if os.path.exists(TRAINING_STATE):
    print(" Reanudando estado previo...")
    try:
        checkpoint = torch.load(TRAINING_STATE, map_location=device)
        model.load_state_dict(checkpoint['model_state'])
        classifier.load_state_dict(checkpoint['classifier_state'])
        optimizer.load_state_dict(checkpoint['optimizer_state'])
        start_epoch = checkpoint['epoch']
        best_loss = checkpoint['best_loss']
        if 'loss_history' in checkpoint: loss_history = checkpoint['loss_history']
        print(f"   -> Epoch inicial: {start_epoch + 1}")
    except:
        print(" Estado corrupto. Iniciando desde cero.")
elif os.path.exists(MODEL_INIT_CKPT):
    model.load_state_dict(torch.load(MODEL_INIT_CKPT, map_location=device))
    print(" Iniciando desde cero.")

# 6. Loop de Entrenamiento
EPOCHS = 3

print("\n" + "="*40)
print(f"  INICIANDO TRAINING (Epochs: {EPOCHS})")
print("="*40)

model.train()
classifier.train()

for epoch in range(start_epoch, start_epoch + EPOCHS):
    running_loss = 0.0
    progress_bar = tqdm(loader, desc=f"Epoch {epoch+1}", leave=True)

    for batch_idx, (specs, speakers, _) in enumerate(progress_bar):
        specs = specs.to(device).float()
        speakers = speakers.to(device).long()

        optimizer.zero_grad()

        # Mixed Precision
        with torch.amp.autocast('cuda', enabled=(device=="cuda")):
            embeddings = model(specs)
            logits = classifier(embeddings)
            loss = criterion(logits, speakers)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        current_loss = loss.item()
        running_loss += current_loss

        if batch_idx % 10 == 0:
            progress_bar.set_postfix({'loss': f"{current_loss:.4f}"})

        # Limpieza periódica de memoria (opcional pero útil)
        if batch_idx % 100 == 0:
            del specs, speakers, embeddings, logits, loss
            torch.cuda.empty_cache()

    # Fin de Epoch
    epoch_avg_loss = running_loss / len(loader)
    loss_history.append(epoch_avg_loss)

    print(f" Epoch {epoch+1} Finalizado | Loss: {epoch_avg_loss:.4f}")

    if epoch_avg_loss < best_loss:
        best_loss = epoch_avg_loss
        torch.save(model.state_dict(), MODEL_BEST)
        print("    Nuevo mejor modelo guardado.")

    state = {
        'epoch': epoch + 1,
        'model_state': model.state_dict(),
        'classifier_state': classifier.state_dict(),
        'optimizer_state': optimizer.state_dict(),
        'best_loss': best_loss,
        'loss_history': loss_history
    }
    torch.save(state, TRAINING_STATE)
    torch.save(model.state_dict(), MODEL_LAST)

print("\n Entrenamiento completado.")

if len(loss_history) > 0:
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o')
    plt.title("Training Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

"""# **BLOQUE 9 — Inferencia, Clustering y Evaluación (DER) EJERCICIO 4 **

Este bloque corresponde a la etapa final de validación del sistema (Ejercicios 3 y 4).

### **Pasos que realiza:**
1.  **Carga del Modelo:** Recupera los pesos de `model_best.pt` (tu mejor época de entrenamiento).
2.  **Inferencia:** Recorre cada audio del set de validación, extrae los segmentos y los pasa por la red neuronal para obtener **embeddings** (vectores de 512 dimensiones).
3.  **Clustering:** Utiliza **Agglomerative Clustering** para agrupar esos vectores. Vectores cercanos pertenecen al mismo hablante.
4.  **Generación de RTTM:** Crea archivos `.rttm` con las predicciones del modelo ("quién habló cuándo").
5.  **Cálculo de DER:** Compara tus predicciones con el "Ground Truth" real y calcula el porcentaje de error.

> **Nota:** Un DER bajo (cercano a 0%) es ideal. En este contexto académico, cualquier valor por debajo del 40-50% indica que el modelo aprendió satisfactoriamente.

## **Importación de Librerías para Evaluación y Métricas**

En esta sección se inicializan todos los módulos necesarios para la fase de inferencia, clustering y cálculo de error.

### **Función de cada librería:**

* **`pyannote.core`**: Proporciona las estructuras de datos fundamentales (`Annotation`, `Segment`) para manejar la cronología y las etiquetas de la diarización.
* **`DiarizationErrorRate` / `JaccardErrorRate`**: Clases esenciales de `pyannote.metrics` que utilizaremos para calcular la nota final del modelo (DER y JER).
* **`AgglomerativeClustering`**: Herramienta de *Scikit-learn* para realizar el agrupamiento jerárquico de los embeddings (vectores de voz), decidiendo qué segmentos pertenecen a la misma persona.
* **`torch`, `numpy`, `pickle`**: Herramientas estándar para la gestión de tensores, operaciones matemáticas (como la normalización de embeddings) y la carga de los datos procesados.
"""

import os
import torch
import numpy as np
import pickle
import warnings
from tqdm.notebook import tqdm
from pyannote.core import Segment, Annotation
from pyannote.metrics.diarization import DiarizationErrorRate, JaccardErrorRate
from sklearn.cluster import AgglomerativeClustering

# Ignorar advertencias no críticas de pyannote
warnings.filterwarnings("ignore", category=UserWarning)

"""### **Configuración de Rutas, Dispositivo y Preparación**

En esta celda se establecen los parámetros iniciales y las rutas necesarias para ejecutar la evaluación del modelo:

1.  **Mecanismo de Recuperación:**
    * El bloque condicional (`if 'CKPT_DIR' not in globals()`) actúa como un **mecanismo de seguridad** contra el reinicio del entorno, redefiniendo las rutas maestras a Google Drive si se perdieron de la memoria RAM.

2.  **Definición de Archivos Críticos:**
    * Se definen las rutas de los archivos esenciales para la evaluación:
        * **Entrada (Input):** `SEGMENTS_CKPT` (metadatos de los segmentos) y `RTTM_DEV_DIR` (etiquetas reales).
        * **Modelo:** `MODEL_BEST.pt` (los pesos del modelo con el mejor rendimiento).
        * **Salida (Output):** `RTTM_PRED_DIR`, que es el directorio donde se guardarán los archivos `.rttm` con las predicciones de hablantes generadas por el sistema.

3.  **Selección de Hardware:**
    * Se configura la variable `device` para priorizar el uso de **`cuda`** (GPU) si está disponible, garantizando la máxima velocidad durante el proceso de inferencia masiva.
"""

# 1. Configuración de Rutas y Dispositivo
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    RTTM_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/rttm/dev"
    SEGMENTS_CKPT = f"{CKPT_DIR}/segments_dev.pkl"

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
RTTM_PRED_DIR = f"{ROOT_DIR}/rttm_pred"
os.makedirs(RTTM_PRED_DIR, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Evaluando en: {device.upper()}")

"""**Carga de Datos, Agrupamiento y Preparación del Modelo**

En esta sección, se prepara la infraestructura de datos y el modelo entrenado para la fase de inferencia:

1.  **Carga de Metadatos:**
    * Se utiliza `pickle.load` para recuperar la lista maestra de segmentos (`segments_list`) generada en el Bloque 6.

2.  **Agrupamiento por URI (Audio):**
    * Se itera sobre toda la lista de segmentos para agruparlos en un diccionario (`segments_by_uri`), donde la clave es el identificador único del audio (URI).
    * **Justificación:** El algoritmo de **Clustering** debe ejecutarse por separado para cada archivo de audio, ya que el modelo debe agrupar los segmentos del audio 'A' sin mezclarlos con los del audio 'B'.

3.  **Carga del Modelo:**
    * Se instancia la clase `SpeakerEmbeddingModel` (ResNet50) y se cargan los pesos del mejor modelo entrenado (`MODEL_BEST.pt`).
    * Se cambia el modo del modelo a **`model.eval()`** para desactivar capas como `Dropout` y el comportamiento dinámico de `BatchNorm`. Esto asegura que las predicciones durante la evaluación sean consistentes y no se vean afectadas por el modo de entrenamiento.
"""

# 2. Cargar Datos y Modelo
print(" Preparando datos...")
with open(SEGMENTS_CKPT, "rb") as f:
    segments_list = pickle.load(f)

# Agrupar segmentos por URI
segments_by_uri = {}
for seg in segments_list:
    uri = os.path.basename(seg["wav_path"]).replace(".wav", "")
    if uri not in segments_by_uri: segments_by_uri[uri] = []
    segments_by_uri[uri].append(seg)

# Cargar Modelo
model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
if os.path.exists(MODEL_BEST):
    model.load_state_dict(torch.load(MODEL_BEST, map_location=device))
    print(" Modelo 'Best' cargado.")
else:
    print(" Usando modelo actual.")
model.eval()

"""**Funciones Auxiliares de Inferencia y Clustering**

En esta sección se definen las tres funciones principales que orquestan la evaluación del modelo, utilizando las clases y el modelo entrenado en bloques anteriores:

1.  **`parse_rttm_file`**:
    * [cite_start]**Función:** Lee los archivos de referencia (Ground Truth) en formato `.rttm` [cite: 23, 24] [cite_start]y los convierte en el objeto `Annotation` [cite: 26] [cite_start]de PyAnnote, extrayendo el tiempo de inicio, la duración y la etiqueta del hablante[cite: 24].

2.  **`get_embeddings_for_audio`**:
    * **Función:** Ejecuta el modelo entrenado en modo de **inferencia**.
    * [cite_start]**Lógica:** Recibe los segmentos de un audio, los procesa en lotes pequeños utilizando un `DataLoader` temporal y, con **`torch.no_grad()`**, pasa los espectrogramas por la ResNet, devolviendo los **vectores de embedding** de 512 dimensiones [cite: 138, 139] sin calcular gradientes.

3.  **`cluster_and_predict`**:
    * **Función:** Convierte los embeddings en etiquetas de hablante predichas.
    * **Pasos:**
        * [cite_start]**Normalización:** Normaliza los embeddings utilizando la norma $L2$ (norma unitaria) [cite: 139] [cite_start]para preparar los datos para la métrica de distancia[cite: 77].
        * [cite_start]**Clustering:** Aplica el algoritmo **Agglomerative Clustering** [cite: 150] para agrupar los vectores. [cite_start]El `distance_threshold` actúa como el parámetro que define cuán diferentes deben ser dos voces para ser consideradas hablantes distintos[cite: 150].
        * **Output:** Construye el objeto `Annotation` (Hipótesis) con los segmentos temporales y las nuevas etiquetas predichas (`spk_0`, `spk_1`, etc.).
"""

# 3. Funciones Auxiliares
def parse_rttm_file(rttm_path, uri):
    annotation = Annotation(uri=uri)
    with open(rttm_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 9 and parts[0] == 'SPEAKER':
                start = float(parts[3])
                duration = float(parts[4])
                if duration > 0:
                    annotation[Segment(start, start + duration)] = parts[7]
    return annotation

def get_embeddings_for_audio(audio_segments, model, device):
    if not audio_segments: return None
    temp_ds = VoxConverseDataset(audio_segments)
    temp_loader = torch.utils.data.DataLoader(temp_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)

    embeddings = []
    with torch.no_grad():
        for specs, _, _ in temp_loader:
            specs = specs.to(device).float()
            if specs.shape[-1] > 0: # Check anti-crash
                emb = model(specs)
                embeddings.append(emb.cpu().numpy())

    if not embeddings: return None
    return np.vstack(embeddings)

def cluster_and_predict(uri, segments, embeddings):
    # Normalización L2 (Cumple consigna de métricas de coseno/euclídea)
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    X = embeddings / (norms + 1e-8)

    # Clustering
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1.0,
        metric='euclidean',
        linkage='ward'
    )
    try:
        labels = clustering.fit_predict(X)
    except:
        labels = np.zeros(len(X), dtype=int)

    hypothesis = Annotation(uri=uri)
    for i, seg in enumerate(segments):
        label = f"spk_{labels[i]}"
        hypothesis[Segment(seg['start'], seg['end'])] = label

    return hypothesis

"""### **4. Bucle Principal de Evaluación**

En esta sección se ejecuta el proceso iterativo que valida el rendimiento del modelo sobre todo el conjunto de datos de desarrollo.

**Flujo de la Evaluación:**
1.  **Inicialización de Métricas:** Se instancian las métricas `DiarizationErrorRate` y `JaccardErrorRate` (DER y JER). El objeto `metric_der` es crucial, ya que **acumula internamente** los componentes de error (confusión, omisión, falsa alarma) a lo largo de todos los archivos.
2.  **Iteración y Proceso:** Se recorre cada audio (URI) del set de desarrollo, utilizando la barra de progreso `tqdm` para monitorizar el avance.
3.  **Comparación:** Por cada archivo:
    * Se carga la **referencia** (`reference`) real desde el archivo `.RTTM`.
    * Se genera la **hipótesis** (`hypothesis`) predicha por el modelo y el *Clustering*.
    * Se guarda el archivo `.RTTM` de la predicción en el directorio de salida (`RTTM_PRED_DIR`).
4.  **Acumulación:** Se invoca a `metric_der` y `metric_jer` para comparar la hipótesis contra la referencia. Los resultados se acumulan en las variables correspondientes para obtener un promedio global al finalizar.
"""

# 4. Loop de Evaluación
# Importante: metric_der acumula los errores de todos los archivos
metric_der = DiarizationErrorRate()
metric_jer = JaccardErrorRate()

# Listas para promedios simples (opcional, pero útil para JER)
jer_scores = []

print(f"\n Evaluando {len(segments_by_uri)} audios...")

for uri, segments in tqdm(segments_by_uri.items(), desc="Calculando Métricas"):
    # Cargar referencia
    rttm_path = os.path.join(RTTM_DEV_DIR, f"{uri}.rttm")
    if not os.path.exists(rttm_path): continue
    reference = parse_rttm_file(rttm_path, uri)

    # Inferencia
    embeddings = get_embeddings_for_audio(segments, model, device)
    if embeddings is None: continue

    hypothesis = cluster_and_predict(uri, segments, embeddings)

    # Guardar predicción
    with open(f"{RTTM_PRED_DIR}/{uri}.rttm", "w") as f:
        hypothesis.write_rttm(f)

    # Calcular Métricas
    try:
        # Al llamar a la métrica, acumula internamente componentes (confusion, miss, etc.)
        _ = metric_der(reference, hypothesis)

        # JER no acumula componentes igual, guardamos el score individual
        jer = metric_jer(reference, hypothesis)
        jer_scores.append(jer)

    except Exception:
        pass

"""**5. Reporte Final de Evaluación y Desglose de Errores**

1.  **Cálculo de Métricas Globales:** Se utiliza el objeto acumulador de métricas (`metric_der`) para obtener el valor final y global de **DER** (Diarization Error Rate).
2.  **Desglose de Componentes:** Se calcula la tasa de error por cada componente solicitado en la consigna, dividiendo el valor acumulado (confusión, omisión, falsa alarma) por el tiempo total de habla (`total_speech`).
    * **Justificación:** Este desglose es clave para entender si el error del modelo proviene de **identidad** (Confusion Rate) o de **detección de voz** (Miss/False Alarm).
3.  **Reporte Final:** Se imprime el resultado del **DER** y **JER** (Jaccard Error Rate) junto con las tasas de error detalladas, lo que proporciona una evaluación completa del desempeño del sistema.
"""

# 5. Resultados Finales Detallados
print("\n" + "="*40)
print("  RESULTADO FINAL")
print("="*40)

# Usamos el acumulado interno de metric_der para sacar los componentes exactos
total_speech = metric_der['total']

if total_speech > 0:
    # Cálculo manual de componentes globales
    global_der = abs(metric_der)
    confusion = metric_der['confusion'] / total_speech
    miss = metric_der['missed detection'] / total_speech
    false_alarm = metric_der['false alarm'] / total_speech

    avg_jer = np.mean(jer_scores) * 100 if jer_scores else 0.0

    print(f" DER Global:         {global_der * 100:.2f}%")
    print(f" JER Promedio:       {avg_jer:.2f}%")
    print("-" * 30)
    print(" Desglose del Error (Componentes):")
    print(f"   • Confusion Rate:   {confusion * 100:.2f}%")
    print(f"   • Miss Rate:        {miss * 100:.2f}%")
    print(f"   • False Alarm Rate: {false_alarm * 100:.2f}%")
    print("-" * 30)
    print(f" Archivos evaluados: {len(jer_scores)}")
    print(f" Predicciones en: {RTTM_PRED_DIR}")

    if global_der < 0.5:
        print(" Buen trabajo. El modelo funciona.")
    else:
        print(" El modelo necesita ajustes.")
else:
    print(" No se pudo calcular las métricas (Total speech = 0).")

"""# **Bloque de código completo para su ejecución**"""

# ===== BLOQUE 9 — Inferencia y Evaluación Completa (DER, JER y Desglose) =====

import os
import torch
import numpy as np
import pickle
import warnings
from tqdm.notebook import tqdm
from pyannote.core import Segment, Annotation
from pyannote.metrics.diarization import DiarizationErrorRate, JaccardErrorRate
from sklearn.cluster import AgglomerativeClustering

# Ignorar advertencias no críticas de pyannote
warnings.filterwarnings("ignore", category=UserWarning)

# 1. Configuración de Rutas y Dispositivo
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"
    RTTM_DEV_DIR = f"{ROOT_DIR}/datasets/voxconverse/rttm/dev"
    SEGMENTS_CKPT = f"{CKPT_DIR}/segments_dev.pkl"

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
RTTM_PRED_DIR = f"{ROOT_DIR}/rttm_pred"
os.makedirs(RTTM_PRED_DIR, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Evaluando en: {device.upper()}")


# 2. Cargar Datos y Modelo
print(" Preparando datos...")
with open(SEGMENTS_CKPT, "rb") as f:
    segments_list = pickle.load(f)

# Agrupar segmentos por URI
segments_by_uri = {}
for seg in segments_list:
    uri = os.path.basename(seg["wav_path"]).replace(".wav", "")
    if uri not in segments_by_uri: segments_by_uri[uri] = []
    segments_by_uri[uri].append(seg)

# Cargar Modelo
model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
if os.path.exists(MODEL_BEST):
    model.load_state_dict(torch.load(MODEL_BEST, map_location=device))
    print(" Modelo 'Best' cargado.")
else:
    print(" Usando modelo actual.")
model.eval()


# 3. Funciones Auxiliares
def parse_rttm_file(rttm_path, uri):
    annotation = Annotation(uri=uri)
    with open(rttm_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 9 and parts[0] == 'SPEAKER':
                start = float(parts[3])
                duration = float(parts[4])
                if duration > 0:
                    annotation[Segment(start, start + duration)] = parts[7]
    return annotation

def get_embeddings_for_audio(audio_segments, model, device):
    if not audio_segments: return None
    temp_ds = VoxConverseDataset(audio_segments)
    temp_loader = torch.utils.data.DataLoader(temp_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)

    embeddings = []
    with torch.no_grad():
        for specs, _, _ in temp_loader:
            specs = specs.to(device).float()
            if specs.shape[-1] > 0: # Check anti-crash
                emb = model(specs)
                embeddings.append(emb.cpu().numpy())

    if not embeddings: return None
    return np.vstack(embeddings)

def cluster_and_predict(uri, segments, embeddings):
    # Normalización L2 (Cumple consigna de métricas de coseno/euclídea)
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    X = embeddings / (norms + 1e-8)

    # Clustering
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1.0,
        metric='euclidean',
        linkage='ward'
    )
    try:
        labels = clustering.fit_predict(X)
    except:
        labels = np.zeros(len(X), dtype=int)

    hypothesis = Annotation(uri=uri)
    for i, seg in enumerate(segments):
        label = f"spk_{labels[i]}"
        hypothesis[Segment(seg['start'], seg['end'])] = label

    return hypothesis


# 4. Loop de Evaluación
# Importante: metric_der acumula los errores de todos los archivos
metric_der = DiarizationErrorRate()
metric_jer = JaccardErrorRate()

# Listas para promedios simples (opcional, pero útil para JER)
jer_scores = []

print(f"\n Evaluando {len(segments_by_uri)} audios...")

for uri, segments in tqdm(segments_by_uri.items(), desc="Calculando Métricas"):
    # Cargar referencia
    rttm_path = os.path.join(RTTM_DEV_DIR, f"{uri}.rttm")
    if not os.path.exists(rttm_path): continue
    reference = parse_rttm_file(rttm_path, uri)

    # Inferencia
    embeddings = get_embeddings_for_audio(segments, model, device)
    if embeddings is None: continue

    hypothesis = cluster_and_predict(uri, segments, embeddings)

    # Guardar predicción
    with open(f"{RTTM_PRED_DIR}/{uri}.rttm", "w") as f:
        hypothesis.write_rttm(f)

    # Calcular Métricas
    try:
        # Al llamar a la métrica, acumula internamente componentes (confusion, miss, etc.)
        _ = metric_der(reference, hypothesis)

        # JER no acumula componentes igual, guardamos el score individual
        jer = metric_jer(reference, hypothesis)
        jer_scores.append(jer)

    except Exception:
        pass

# 5. Resultados Finales Detallados
print("\n" + "="*40)
print("  RESULTADO FINAL")
print("="*40)

# Usamos el acumulado interno de metric_der para sacar los componentes exactos
total_speech = metric_der['total']

if total_speech > 0:
    # Cálculo manual de componentes globales
    global_der = abs(metric_der)
    confusion = metric_der['confusion'] / total_speech
    miss = metric_der['missed detection'] / total_speech
    false_alarm = metric_der['false alarm'] / total_speech

    avg_jer = np.mean(jer_scores) * 100 if jer_scores else 0.0

    print(f" DER Global:         {global_der * 100:.2f}%")
    print(f" JER Promedio:       {avg_jer:.2f}%")
    print("-" * 30)
    print(" Desglose del Error (Componentes):")
    print(f"   • Confusion Rate:   {confusion * 100:.2f}%")
    print(f"   • Miss Rate:        {miss * 100:.2f}%")
    print(f"   • False Alarm Rate: {false_alarm * 100:.2f}%")
    print("-" * 30)
    print(f" Archivos evaluados: {len(jer_scores)}")
    print(f" Predicciones en: {RTTM_PRED_DIR}")

    if global_der < 0.5:
        print(" Buen trabajo. El modelo funciona.")
    else:
        print(" El modelo necesita ajustes.")
else:
    print(" No se pudo calcular las métricas (Total speech = 0).")

"""# **BLOQUE 10 — Testing con Archivos Reales (Ejercicio 4)**

En este bloque realizamos la validación cualitativa del modelo utilizando audios del "mundo real" (fuera del dataset VoxConverse).

Debido a las restricciones actuales de descarga directa desde YouTube, se implementa un flujo de trabajo basado en **archivos locales** (ej. audios extraídos de TikTok o grabaciones propias en formato `.mp3` o `.wav`).

### **Metodología Implementada:**
1.  **Ingesta de Datos:** El sistema busca archivos de audio en la carpeta `mis_audios_test` de Google Drive.
2.  **Preprocesamiento:**
    * Carga del audio y re-muestreo a **16kHz** (frecuencia esperada por el modelo).
    * **Normalización** de amplitud.
    * **Segmentación (Chunking):** División del audio en ventanas deslizantes para procesar secuencias largas.
3.  **Inferencia:**
    * Extracción de embeddings con **ResNet50**.
    * Agrupamiento de hablantes mediante **Clustering**.
4.  **Resultado:** Generación de una transcripción temporal ("Quién habla cuándo") y reproducción del audio para verificación manual.

**Configuración de Librerías para Pruebas**

En esta celda inicializamos las herramientas esenciales necesarias para la manipulación y ejecución de la prueba cualitativa (Ejercicio 4), adaptada a archivos locales:

* **`librosa`**: Librería principal para la carga de archivos de audio (`.mp3`, `.wav`) y su re-muestreo a 16kHz.
* **`pydub`**: Librería de apoyo para la manipulación y conversión de formatos de audio.
* **`torch`, `numpy`**: Módulos esenciales para cargar el modelo en GPU/CPU, realizar operaciones numéricas y manejar tensores.
* **`IPython.display` (`Audio`, `display`)**: Elementos cruciales para la demostración en vivo, ya que nos permiten reproducir el audio de prueba y visualizar los resultados directamente en el notebook.
* **`pyannote.core`**: Nos permite estructurar la salida de la diarización (`Annotation`) con los segmentos de tiempo predichos.
"""

# 1. Instalación de librerías
print(" Verificando librerías...")
!pip install -q librosa pydub

import os
import torch
import librosa
import numpy as np
from IPython.display import Audio, display
from pyannote.core import Segment, Annotation

"""**Configuración de Rutas de Prueba y Hardware**

En esta sección inicializamos las variables de entorno para el testing cualitativo (Ejercicio 4) y definimos el hardware de ejecución:

1.  **Mecanismo de Recuperación:**
    * El bloque condicional (`if 'CKPT_DIR' not in globals()`) verifica si las variables de ruta maestras existen en la memoria. Si se perdieron por un reinicio, las restablece automáticamente, garantizando la **robustez** del bloque.

2.  **Definición del Entorno de Pruebas:**
    * Se define `TEST_FILES_DIR`, la ubicación exacta en Google Drive donde se alojarán los audios externos (MP3/WAV de TikTok, grabaciones, etc.).
    * Se crea la carpeta usando `os.makedirs(exist_ok=True)`.

3.  **Ruta del Modelo y Hardware:**
    * Se define `MODEL_BEST`, que apunta a los pesos entrenados del modelo con mejor rendimiento.
    * Se configura la variable `device`, priorizando la **GPU (`cuda`)** para la inferencia de embeddings por su velocidad.
"""

# 2. Configuración de Rutas
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"

TEST_FILES_DIR = f"{ROOT_DIR}/mis_audios_test"
os.makedirs(TEST_FILES_DIR, exist_ok=True)

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f" Procesando en: {device.upper()}")
print(f" Carpeta de prueba: {TEST_FILES_DIR}")

"""**3. Funciones de Inferencia y Adaptación Local**

Esta sección define el *pipeline* completo para procesar audios externos que no tienen etiquetas RTTM. Su diseño es clave para reutilizar el modelo entrenado con datos provenientes de fuentes como TikTok o grabaciones locales.

**1. `diarize_local_audio` (Función Principal):**
* **Adaptación Crítica:** Utiliza `librosa.load(..., mono=True)` para **forzar la conversión a un solo canal**. Esto corrige el error de dimensiones que se presentaba al intentar procesar archivos MP3 estéreo con un modelo entrenado para un solo canal de audio.
* **Chunking:** Genera segmentos de tiempo (`segments`) mediante una ventana deslizante sobre la duración total del audio.

**2. `LocalInferenceDataset` (Clase Anidada):**
* **Propósito:** Es una versión ligera y robusta del `Dataset` del Bloque 6, definida localmente para el Bloque 10.
* **Funcionalidad:** Implementa la lógica de carga (`torchaudio.load`), re-muestreo, corte y **conversión forzada a mono** (`torch.mean(wav, dim=0)`). Esto garantiza que el tensor de entrada siempre tenga la forma `[1, n_mels, T]` que espera la ResNet.

**3. `get_local_embeddings`:**
* **Inferencia:** Utiliza el `DataLoader` y el modelo en modo `torch.no_grad()` para extraer los vectores de embedding (huellas de voz) de cada segmento a la máxima velocidad posible.

**4. `cluster_and_predict`:**
* **Agrupamiento:** Aplica la lógica de *Agglomerative Clustering* (definida en Bloque 9) sobre los embeddings resultantes para asignar etiquetas de hablante (`spk_0`, `spk_1`).
"""

# 3. Funciones de Procesamiento

def diarize_local_audio(wav_path, model, device, window=1.5, step=0.75):
    """
    Pipeline completo: Carga -> Chunking -> Embedding -> Clustering.
    """
    filename = os.path.basename(wav_path)
    print(f" Analizando estructura de: {filename}...")

    # A) Cargar audio forzando MONO (mono=True)
    try:
        # Importante: mono=True mezcla los canales estéreo en uno solo
        wav, sr = librosa.load(wav_path, sr=16000, mono=True)
    except Exception as e:
        print(f" Error cargando audio: {e}")
        return None

    total_duration = librosa.get_duration(y=wav, sr=sr)
    print(f"    Duración total: {total_duration:.2f} segundos.")

    # B) Chunking (Ventana deslizante)
    segments = []
    for start in np.arange(0, total_duration - window, step):
        segments.append({
            "wav_path": wav_path,
            "start": start,
            "end": start + window,
            "speaker_idx": 0
        })

    # C) Inferencia (Embeddings)
    # Verificamos que las funciones necesarias existan
    if 'get_embeddings_for_audio' not in globals():
        print(" ERROR: Falta ejecutar el Bloque 9.")
        return None

    # El dataset class también necesita manejar MP3s estéreo si lee directo del disco.
    # Como 'get_embeddings_for_audio' usa 'VoxConverseDataset', debemos asegurarnos
    # de que esa clase cargue bien el audio.
    # TRUCO: Para no redefinir el Dataset del Bloque 6, aquí pasamos los segmentos.
    # VoxConverseDataset usa torchaudio.load(). Torchaudio load devuelve (canales, tiempo).
    # Si el archivo es estéreo, devuelve (2, N). Nuestro Dataset espera (1, N).

    # Por seguridad, definimos un Dataset temporal aquí que FUERZA mono
    # para evitar modificar el Bloque 6 y romper compatibilidad.
    class LocalInferenceDataset(torch.utils.data.Dataset):
        def __init__(self, segments):
            self.segments = segments
            self.mel = torch.nn.Sequential(
                torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False).conv1 # dummy load to ensure torch works
            )
            # Recreamos transformaciones
            self.mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=16000, n_fft=512, hop_length=160, n_mels=64
            )
            self.db_transform = torchaudio.transforms.AmplitudeToDB()

        def __len__(self): return len(self.segments)

        def __getitem__(self, idx):
            item = self.segments[idx]
            # Cargar con torchaudio
            wav, sr = torchaudio.load(item["wav_path"])

            # --- CORRECCIÓN CRÍTICA: FORZAR MONO ---
            if wav.shape[0] > 1:
                wav = torch.mean(wav, dim=0, keepdim=True)

            if sr != 16000:
                wav = torchaudio.transforms.Resample(sr, 16000)(wav)

            start = int(item["start"] * 16000)
            end = int(item["end"] * 16000)
            if end > wav.shape[1]: end = wav.shape[1]

            chunk = wav[:, start:end]
            # Padding si es muy corto
            if chunk.shape[1] < 100:
                return torch.zeros(1, 64, 100), torch.tensor(0)

            mel = self.db_transform(self.mel_transform(chunk))
            return mel, torch.tensor(0)

    # Función local de embeddings que usa el Dataset corregido
    def get_local_embeddings(segs):
        ds = LocalInferenceDataset(segs)
        dl = torch.utils.data.DataLoader(ds, batch_size=32, collate_fn=collate_fn, shuffle=False)
        embs = []
        with torch.no_grad():
            for specs, _, _ in dl:
                specs = specs.to(device).float()
                # Verificar dimensiones (Batch, 1, Mel, Time)
                if specs.dim() == 5: # Si viene [B, 1, 1, 64, T] por error
                    specs = specs.squeeze(2)

                if specs.shape[-1] > 0:
                    e = model(specs)
                    embs.append(e.cpu().numpy())
        if not embs: return None
        return np.vstack(embs)

    embeddings = get_local_embeddings(segments)

    if embeddings is None:
        print(" Advertencia: No se detectó audio válido.")
        return None

    # D) Clustering y Predicción
    hypothesis = cluster_and_predict("local_test", segments, embeddings)

    return hypothesis, wav_path

"""**4. Ejecución del Pipeline y Demostración Cualitativa**

En esta celda se orquesta la prueba final del sistema, ejecutando la inferencia completa sobre un archivo de audio local seleccionado:

1.  **Descubrimiento y Selección:** El código escanea la carpeta `TEST_FILES_DIR` para listar todos los archivos de audio disponibles. Por defecto, se selecciona el primer archivo (`SELECCION = 0`) para la demostración.

2.  **Carga del Modelo Final:** Se instancia el modelo y se carga el archivo de pesos `MODEL_BEST.pt` (los pesos que arrojaron el menor error durante el entrenamiento). Es esencial llamar a **`model.eval()`** para asegurar que el modelo se comporte de forma consistente (desactivando capas como `Dropout`).

3.  **Ejecución del Pipeline:** Se llama a la función `diarize_local_audio` para procesar el archivo.

4.  **Validación y Demostración:**
    * **Reproductor:** Se utiliza `IPython.display.Audio` para permitir la reproducción del audio dentro del notebook.
    * **Transcripción Temporal:** Se imprime una lista detallada (`[HH:MM:SS]`) de los segmentos de habla y sus etiquetas predichas (`spk_0`, `spk_1`). Esto permite la **auditoría manual** y la verificación cualitativa del rendimiento del modelo en entornos reales.
"""

# --- 4. EJECUCIÓN PRINCIPAL ---

print("\n Buscando archivos .mp3 / .wav...")
files = sorted([f for f in os.listdir(TEST_FILES_DIR) if f.endswith(('.mp3', '.wav', '.m4a'))])

if not files:
    print(f" CARPETA VACÍA: {TEST_FILES_DIR}")
else:
    # SELECCIÓN AUTOMÁTICA O MANUAL
    SELECCION = 0

    print(f" Se encontraron {len(files)} archivos.")
    print(f" Procesando archivo [{SELECCION}]: {files[SELECCION]}")

    filename = files[SELECCION]
    full_path = os.path.join(TEST_FILES_DIR, filename)

    try:
        # Cargar Modelo
        model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
        if os.path.exists(MODEL_BEST):
            model.load_state_dict(torch.load(MODEL_BEST, map_location=device))
        model.eval()

        # Ejecutar
        result_tuple = diarize_local_audio(full_path, model, device)

        if result_tuple:
            result, loaded_path = result_tuple

            # Reproducir
            print("\n Audio Original:")
            display(Audio(loaded_path, rate=16000))

            # Resultados
            print("\n" + "="*50)
            print(f"  TRANSCRIPCIÓN: {filename}")
            print("="*50)

            count = 0
            for segment, track, label in result.itertracks(yield_label=True):
                s_min, s_sec = divmod(segment.start, 60)
                e_min, e_sec = divmod(segment.end, 60)
                print(f"⏱️ [{int(s_min):02d}:{s_sec:05.2f} - {int(e_min):02d}:{e_sec:05.2f}] \t🗣️ {label}")
                count += 1

            if count == 0:
                print(" El modelo no generó segmentos (posiblemente todo silencio o error de umbral).")

    except Exception as e:
        print(f" Error inesperado: {e}")
        import traceback
        traceback.print_exc()

"""# **Bloque de código completo ejecutado**"""

# ===== BLOQUE 10 — Testing con Archivos Locales =====

# 1. Instalación de librerías
print(" Verificando librerías...")
!pip install -q librosa pydub

import os
import torch
import librosa
import numpy as np
from IPython.display import Audio, display
from pyannote.core import Segment, Annotation

# 2. Configuración de Rutas
if 'CKPT_DIR' not in globals():
    ROOT_DIR = "/content/drive/MyDrive/TP_FINAL_DIARIZATION"
    CKPT_DIR = f"{ROOT_DIR}/checkpoints"

TEST_FILES_DIR = f"{ROOT_DIR}/mis_audios_test"
os.makedirs(TEST_FILES_DIR, exist_ok=True)

MODEL_BEST = f"{CKPT_DIR}/model_best.pt"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f" Procesando en: {device.upper()}")
print(f" Carpeta de prueba: {TEST_FILES_DIR}")

# 3. Funciones de Procesamiento

def diarize_local_audio(wav_path, model, device, window=1.5, step=0.75):
    """
    Pipeline completo: Carga -> Chunking -> Embedding -> Clustering.
    """
    filename = os.path.basename(wav_path)
    print(f" Analizando estructura de: {filename}...")

    # A) Cargar audio forzando MONO (mono=True)
    try:
        # Importante: mono=True mezcla los canales estéreo en uno solo
        wav, sr = librosa.load(wav_path, sr=16000, mono=True)
    except Exception as e:
        print(f" Error cargando audio: {e}")
        return None

    total_duration = librosa.get_duration(y=wav, sr=sr)
    print(f"    Duración total: {total_duration:.2f} segundos.")

    # B) Chunking (Ventana deslizante)
    segments = []
    for start in np.arange(0, total_duration - window, step):
        segments.append({
            "wav_path": wav_path,
            "start": start,
            "end": start + window,
            "speaker_idx": 0
        })

    # C) Inferencia (Embeddings)
    # Verificamos que las funciones necesarias existan
    if 'get_embeddings_for_audio' not in globals():
        print(" ERROR: Falta ejecutar el Bloque 9.")
        return None

    # El dataset class también necesita manejar MP3s estéreo si lee directo del disco.
    # Como 'get_embeddings_for_audio' usa 'VoxConverseDataset', debemos asegurarnos
    # de que esa clase cargue bien el audio.
    # TRUCO: Para no redefinir el Dataset del Bloque 6, aquí pasamos los segmentos.
    # VoxConverseDataset usa torchaudio.load(). Torchaudio load devuelve (canales, tiempo).
    # Si el archivo es estéreo, devuelve (2, N). Nuestro Dataset espera (1, N).

    # Por seguridad, definimos un Dataset temporal aquí que FUERZA mono
    # para evitar modificar el Bloque 6 y romper compatibilidad.
    class LocalInferenceDataset(torch.utils.data.Dataset):
        def __init__(self, segments):
            self.segments = segments
            self.mel = torch.nn.Sequential(
                torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False).conv1 # dummy load to ensure torch works
            )
            # Recreamos transformaciones
            self.mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=16000, n_fft=512, hop_length=160, n_mels=64
            )
            self.db_transform = torchaudio.transforms.AmplitudeToDB()

        def __len__(self): return len(self.segments)

        def __getitem__(self, idx):
            item = self.segments[idx]
            # Cargar con torchaudio
            wav, sr = torchaudio.load(item["wav_path"])

            # --- CORRECCIÓN CRÍTICA: FORZAR MONO ---
            if wav.shape[0] > 1:
                wav = torch.mean(wav, dim=0, keepdim=True)

            if sr != 16000:
                wav = torchaudio.transforms.Resample(sr, 16000)(wav)

            start = int(item["start"] * 16000)
            end = int(item["end"] * 16000)
            if end > wav.shape[1]: end = wav.shape[1]

            chunk = wav[:, start:end]
            # Padding si es muy corto
            if chunk.shape[1] < 100:
                return torch.zeros(1, 64, 100), torch.tensor(0)

            mel = self.db_transform(self.mel_transform(chunk))
            return mel, torch.tensor(0)

    # Función local de embeddings que usa el Dataset corregido
    def get_local_embeddings(segs):
        ds = LocalInferenceDataset(segs)
        dl = torch.utils.data.DataLoader(ds, batch_size=32, collate_fn=collate_fn, shuffle=False)
        embs = []
        with torch.no_grad():
            for specs, _, _ in dl:
                specs = specs.to(device).float()
                # Verificar dimensiones (Batch, 1, Mel, Time)
                if specs.dim() == 5: # Si viene [B, 1, 1, 64, T] por error
                    specs = specs.squeeze(2)

                if specs.shape[-1] > 0:
                    e = model(specs)
                    embs.append(e.cpu().numpy())
        if not embs: return None
        return np.vstack(embs)

    embeddings = get_local_embeddings(segments)

    if embeddings is None:
        print(" Advertencia: No se detectó audio válido.")
        return None

    # D) Clustering y Predicción
    hypothesis = cluster_and_predict("local_test", segments, embeddings)

    return hypothesis, wav_path

# --- 4. EJECUCIÓN PRINCIPAL ---

print("\n Buscando archivos .mp3 / .wav...")
files = sorted([f for f in os.listdir(TEST_FILES_DIR) if f.endswith(('.mp3', '.wav', '.m4a'))])

if not files:
    print(f" CARPETA VACÍA: {TEST_FILES_DIR}")
else:
    # SELECCIÓN AUTOMÁTICA O MANUAL
    SELECCION = 0

    print(f" Se encontraron {len(files)} archivos.")
    print(f" Procesando archivo [{SELECCION}]: {files[SELECCION]}")

    filename = files[SELECCION]
    full_path = os.path.join(TEST_FILES_DIR, filename)

    try:
        # Cargar Modelo
        model = SpeakerEmbeddingModel(embedding_dim=512).to(device)
        if os.path.exists(MODEL_BEST):
            model.load_state_dict(torch.load(MODEL_BEST, map_location=device))
        model.eval()

        # Ejecutar
        result_tuple = diarize_local_audio(full_path, model, device)

        if result_tuple:
            result, loaded_path = result_tuple

            # Reproducir
            print("\n Audio Original:")
            display(Audio(loaded_path, rate=16000))

            # Resultados
            print("\n" + "="*50)
            print(f"  TRANSCRIPCIÓN: {filename}")
            print("="*50)

            count = 0
            for segment, track, label in result.itertracks(yield_label=True):
                s_min, s_sec = divmod(segment.start, 60)
                e_min, e_sec = divmod(segment.end, 60)
                print(f"⏱️ [{int(s_min):02d}:{s_sec:05.2f} - {int(e_min):02d}:{e_sec:05.2f}] \t🗣️ {label}")
                count += 1

            if count == 0:
                print(" El modelo no generó segmentos (posiblemente todo silencio o error de umbral).")

    except Exception as e:
        print(f" Error inesperado: {e}")
        import traceback
        traceback.print_exc()

"""# **BLOQUE FINAL — Conclusiones**

En este Trabajo Práctico Final desarrollé e implementé un sistema completo de **Speaker Diarization** (Diarización de Hablantes) basado en técnicas de Deep Learning.

* **Metodología:** Construí un pipeline que transforma audio crudo en Espectrogramas Mel, extrae características (embeddings) mediante una arquitectura **ResNet50** adaptada a 1 canal, y agrupa las identidades de los hablantes utilizando **Clustering Jerárquico Aglomerativo**.
* **Entrenamiento:** Entrené el modelo utilizando el dataset **VoxConverse** (subset *dev*), optimizando la red para proyectar voces distintas en puntos distantes de un espacio vectorial de 512 dimensiones.

## **2. Análisis de Resultados**
Tras la evaluación sobre 216 audios del set de validación, el sistema alcanzó los siguientes resultados:

* **DER (Diarization Error Rate): 41.27%**
* **JER (Jaccard Error Rate): 52.07%**

**Interpretación del Error:**
Al analizar los componentes del DER, se observa un comportamiento muy particular:
* **Detección de Voz Perfecta:** Tuvimos un *Miss Rate* del **0.29%** y un *False Alarm Rate* del **0.00%**. Esto indica que el sistema es extremadamente preciso detectando la presencia de voz; no pierde información ni alucina voces en el silencio.
* **Desafío de Identidad:** La gran mayoría del error proviene del **Confusion Rate (40.98%)**. Esto significa que el modelo detecta correctamente que alguien habla, pero tiene dificultades para distinguir *quién* es (confunde al Hablante A con el Hablante B). Esto es esperable dado que utilizamos una arquitectura de visión (ResNet) adaptada, en lugar de una específica de audio.

## **3. Desafíos y Soluciones**
Durante el desarrollo, superé varios obstáculos técnicos importantes:
1.  **Gestión de Memoria (OOM):** El procesamiento de audios de larga duración saturaba la memoria de la GPU (T4). Lo solucioné implementando un *Batch Size* reducido (8) y limpieza activa de memoria (`gc.collect`).
2.  **Incompatibilidad de Librerías:** Resolví conflictos críticos entre versiones recientes de `NumPy` y la librería `PyAnnote` mediante la reinstalación controlada del entorno.
3.  **Testing Real:** Adapté el flujo de prueba para funcionar con archivos locales debido a restricciones en la descarga directa de YouTube, logrando validar el modelo con audios externos.



"""